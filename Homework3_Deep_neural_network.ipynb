{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham13596/Stanford-CS224S/blob/main/Homework3_Deep_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfqwLKSPaAw"
      },
      "source": [
        "# CS224S Assignment 3: Deep Learning for End-to-End Speech Recognition\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXsGVZtG-dzr"
      },
      "source": [
        "This notebook is worth 100 of the total 160 possible points for homework 3. You should be able to train all models in Colab. We encourage you to read general PyTorch / Lightning tutorials as necessary as you work. You might need to purchase more Colab GPU credits for training to work reasonably fast.\n",
        "\n",
        "We provide a target error rate for each model training exercise, you should be able to obtain an error rate at least this good using the code setup and data provided. Start early, model training may take 30 mins or more per training run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh8A_5L1dq6-"
      },
      "source": [
        "**Note:** You will need to make a copy of this Colab notebook in your Google Drive before you can edit it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX_ihNthYzUt",
        "outputId": "d817ea4a-6706-40ac-aff0-ae6810e77ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9ahWfoydgHv",
        "outputId": "0c087250-88db-48ee-e580-c27ed21e2cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/cs224s_spring2025/data\n",
            "/content/gdrive/MyDrive/cs224s_spring2025\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Do not modify.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/MyDrive/cs224s_spring2025'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "SYM_PATH = '/content/cs224s_spring2025'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH\n",
        "\n",
        "DATA_PATH = '{}/data'.format(SYM_PATH)\n",
        "if not os.path.exists(DATA_PATH):\n",
        "  %mkdir $DATA_PATH\n",
        "%cd $DATA_PATH\n",
        "if not os.path.exists(os.path.join(DATA_PATH, 'harper_valley_bank_minified')):\n",
        "  !wget -q http://web.stanford.edu/class/cs224s/download/harper_valley_bank_minified.zip\n",
        "  !unzip -q harper_valley_bank_minified.zip\n",
        "  %rm harper_valley_bank_minified.zip\n",
        "\n",
        "MODEL_PATH = '{}/trained_models'.format(SYM_PATH)\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "  %mkdir $MODEL_PATH\n",
        "\n",
        "%cd $SYM_PATH\n",
        "if not os.path.exists(os.path.join(SYM_PATH, 'utils.py')):\n",
        "  !wget -q http://web.stanford.edu/class/cs224s/download/utils.py\n",
        "\n",
        "!pip -q install pytorch_lightning\n",
        "!pip install wandb -qqq\n",
        "\n",
        "from collections import OrderedDict\n",
        "from itertools import chain\n",
        "\n",
        "import h5py\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from glob import glob\n",
        "import librosa\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from sklearn.metrics import f1_score\n",
        "from typing import *\n",
        "from IPython.display import Audio\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7eUiyEhfnYV"
      },
      "source": [
        "# Part 1: ML Speech Data Pipeline\n",
        "\n",
        "HarperValleyBank consists of 23 hours of audio from 1,446 human-human conversations between 59 unique speakers. For your convenience, we store in `harper_valley_bank_minified` all utterance audio waveforms as `numpy` arrays in `data.h5` and all transcripts and labels as `numpy` arrays in `labels.npz`.\n",
        "\n",
        "Our custom dataset class `HarperValleyBank` should inherit `torch.utils.data.Dataset` and overwrite the following methods:\n",
        "- `__len__` so that `len(dataset)` returns the size of the dataset.\n",
        "- `__getitem__` to support the indexing such that `dataset[i]` can be used to get the `i`th dataset sample.\n",
        "\n",
        "There are a few special features that the `HarperValleyBank` class should exhibit.\n",
        "- **Fixed-length data.** Both the extracted audio features and the character labels will inherently be sequences of different lengths. However, in order to store data in a minibatch during training, we need to make the lengths uniform. To do so, we can first enforce a maximum length for audio waves and a maximum length for labels (note that these two maximum lengths are not necessarily the same). We have preprocessed all sequences to be cropped by single utterances as opposed to conversations. Next, we can crop and pad each sequence with a pad token (e.g. `3`) such that all audio sequences and all label sequences are their respective maximum lengths. We will also store the actual lengths of each sequence so that the model does not learn from the padded indices.\n",
        "- **Sequence representation.** We are training a character-level model, so the ASR model is responsible for predicting each spoken character. Therefore, we must convert our transcript text to a list of indices representing 34 possible characters (see the global variable `VOCAB`) and a few domain-specific tokens (see the global variable `SILENT_VOCAB` e.g. `[laughter]`). Think of each character as its own class.\n",
        "```\n",
        "Raw utterance:  hi this is an example .\n",
        "List of characters: ['h', 'i', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', '.']\n",
        "List of indices: [18, 19, 3, 30, 18, 19, 29, 3, 19, 29, 3, 11, 24, 3, 15, 34, 11, 23, 26, 22, 15, 3, 6]\n",
        "```\n",
        "- **Special tokens.** Although this next part is provided, it is worth pointing out. Aside from the padding index, there are three special tokens in our vocabulary:\n",
        "    - A blank token (`epsilon`, represented by index `0`) which designates a padded index and plays a special role in CTC.\n",
        "    - A start-of-sentence token (`SOS`, represented by index `1`) which designates the start of a sentence.\n",
        "    - An end-of-sentence token (`EOS`, represented by index `2`) which designates the end of a sentence.\n",
        "```\n",
        "Example label sequence: [18, 19, 3, 30, 18]\n",
        "Add an END token: [18, 19, 3, 30, 18, 2]\n",
        "```\n",
        "Suppose the maximum label sequence has length 10.\n",
        "```\n",
        "Padded label sequence: [18, 19, 3, 30, 18, 2, 0, 0, 0, 0]\n",
        "Label sequence length: 6\n",
        "```\n",
        "\n",
        "**It may be helpful to first read through the `HarperValleyBank` starter code and `utils.py` to get familiar with the data pipeline.**\n",
        "\n",
        "Below, we provide a cell for you to index into the raw data and listen to randomly chosen samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGmsjnx_karw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "ff778402-bed4-4587-f7aa-bdbfd2445f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index 20242: \"electric\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
        "waveform_h5 = h5py.File(os.path.join(root, 'data.h5'), 'r')\n",
        "waveform_data = waveform_h5.get('waveforms')\n",
        "label_data = np.load(os.path.join(root, 'labels.npz'))\n",
        "assert len(waveform_data) == len(label_data['human_transcripts'])\n",
        "index = random.randint(0, len(waveform_data) - 1)\n",
        "w = waveform_data[f'{index}'][:]\n",
        "t = label_data['human_transcripts'][index]\n",
        "\n",
        "print('index {}: \"{}\"\\n'.format(index, t))\n",
        "Audio(w, rate=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3aU-u6laAzt"
      },
      "source": [
        "## **Task 1.1: Set up primary task data (5 Points)**\n",
        "\n",
        "To train speech recognition models, we need a consistent input format. However, raw audio clips and transcript labels vary in length, which makes batching impossible.\n",
        "\n",
        "Thus, for every dataset sample, we must generate four objects:\n",
        "\n",
        "- `inputs`: the log-Mel spectrogram features, padded to a fixed maximum length.\n",
        "- `input_lengths`: the true (unpadded) length of the spectrogram (in frames).\n",
        "- `labels`: the character-level transcript labels, padded to a fixed maximum length.\n",
        "- `label_lengths`: the true (unpadded) number of label tokens.\n",
        "\n",
        "These objects will be used for what we call our *primary* task: speech recognition. In later parts, we will use *auxiliary* tasks to perform multi-task learning toward boosting speech recognition.\n",
        "\n",
        "**→ Implement the `get_primary_task_data` method.** This will be used in the `__getitem__` method of `HarperValleyBank` and later its subclass for multi-task learning, and it is responsible for extracting log-Mel spectrogram features from the raw audio clips. Do not modify other methods. You should pass the sanity check at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ2x5IuxfuZf"
      },
      "outputs": [],
      "source": [
        "from utils import (\n",
        "  prune_transcripts, pad_wav, pad_transcript_label, get_transcript_labels,\n",
        "  get_cer_per_sample)\n",
        "\n",
        "\n",
        "# HarperValleyBank character vocabulary\n",
        "VOCAB = [' ', \"'\", '~', '-', '.', '<', '>', '[', ']', 'a', 'b', 'c', 'd', 'e',\n",
        "         'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's',\n",
        "         't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "SILENT_VOCAB = ['[baby]', '[ringing]', '[laughter]', '[kids]', '[music]',\n",
        "                '[noise]', '[unintelligible]', '[dogs]', '[cough]']\n",
        "\n",
        "\n",
        "class HarperValleyBank(Dataset):\n",
        "  \"\"\"Dataset to be used to train CTC, LAS, and MTL.\n",
        "\n",
        "  Args:\n",
        "    root: string\n",
        "          path to the data files.\n",
        "    split: string (default: train)\n",
        "            choices: train | val | test\n",
        "            which split of data to load\n",
        "    n_mels: integer (default: 128)\n",
        "            number of mel frequencies\n",
        "    n_fft: integer (default: 256)\n",
        "            number of fourier components\n",
        "    win_length: integer (default: 256)\n",
        "                should be <= n_fft\n",
        "    hop_length: integer (default: 128)\n",
        "                number of frames to skip in between\n",
        "    wav_max_length: integer (default: 200)\n",
        "                    maximum number of timesteps in a waveform\n",
        "    transcript_max_length: integer (default: 200)\n",
        "                            maximum number of timesteps in a transcript\n",
        "    append_eos_token: boolean (default: False)\n",
        "                      add EOS token to the end of every transcription\n",
        "                      this is used for LAS (and LAS+CTC models)\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self, root, split='train', n_mels=128, n_fft=256, win_length=256,\n",
        "      hop_length=128, wav_max_length=200, transcript_max_length=200,\n",
        "      append_eos_token=False):\n",
        "    super().__init__()\n",
        "    print(f'> Constructing HarperValleyBank {split} dataset...')\n",
        "\n",
        "    self.label_data = np.load(os.path.join(root, 'labels.npz'))\n",
        "    self.root = root\n",
        "    self.wav_max_length = wav_max_length\n",
        "    self.transcript_max_length = transcript_max_length\n",
        "\n",
        "    self.input_dim = n_mels\n",
        "    self.n_mels = n_mels\n",
        "    self.n_fft = n_fft\n",
        "    self.win_length = win_length\n",
        "    self.hop_length = hop_length\n",
        "\n",
        "    # Prune away very short examples.\n",
        "    # This returns a list of indices of examples longer than 3 words.\n",
        "    valid_indices = prune_transcripts(self.label_data['human_transcripts'])\n",
        "\n",
        "    # Decides which indices belong to which split.\n",
        "    train_indices, val_indices, test_indices = self.split_data(valid_indices)\n",
        "\n",
        "    if split == 'train':\n",
        "      indices = train_indices\n",
        "    elif split == 'val':\n",
        "      indices = val_indices\n",
        "    elif split == 'test':\n",
        "      indices = test_indices\n",
        "    else:\n",
        "      raise Exception(f'Split {split} not supported.')\n",
        "\n",
        "    raw_human_transcripts = self.label_data['human_transcripts'].tolist()\n",
        "    human_transcript_labels = get_transcript_labels(\n",
        "      raw_human_transcripts, VOCAB, SILENT_VOCAB)\n",
        "\n",
        "    # Increment all indices by 4 to reserve the following special tokens:\n",
        "    #   0 for epsilon\n",
        "    #   1 for start-of-sentence (SOS)\n",
        "    #   2 for end-of-sentence (EOS)\n",
        "    #   3 for padding\n",
        "    num_special_tokens = 4\n",
        "    human_transcript_labels = [list(np.array(lab) + num_special_tokens)\n",
        "                                for lab in human_transcript_labels]\n",
        "    # CTC doesn't use SOS nor EOS; LAS doesn't use EPS but add anyway.\n",
        "    eps_index, sos_index, eos_index, pad_index = 0, 1, 2, 3\n",
        "\n",
        "    if append_eos_token:\n",
        "      # Ensert an EOS token to the end of all the labels.\n",
        "      # This is important for the LAS objective.\n",
        "      human_transcript_labels_ = []\n",
        "      for i in range(len(human_transcript_labels)):\n",
        "        new_label_i = human_transcript_labels[i] + [eos_index]\n",
        "        human_transcript_labels_.append(new_label_i)\n",
        "      human_transcript_labels = human_transcript_labels_\n",
        "    self.human_transcript_labels = human_transcript_labels\n",
        "\n",
        "    # Include epsilon, SOS, and EOS tokens.\n",
        "    self.num_class = len(VOCAB) + len(SILENT_VOCAB) + num_special_tokens\n",
        "    self.num_labels = self.num_class  # These are interchangeable.\n",
        "    self.eps_index = eps_index\n",
        "    self.sos_index = sos_index\n",
        "    self.eos_index = eos_index\n",
        "    self.pad_index = pad_index # Use this index for padding.\n",
        "\n",
        "    self.indices = indices\n",
        "\n",
        "  def indices_to_chars(self, indices):\n",
        "    # indices: list of integers in vocab\n",
        "    # add special characters in front (since we did this above)\n",
        "    full_vocab = ['<eps>', '<sos>', '<eos>', '<pad>'] + VOCAB + SILENT_VOCAB\n",
        "    chars = [full_vocab[ind] for ind in indices]\n",
        "    return chars\n",
        "\n",
        "  def split_data(self, valid_indices, train_ratio = 0.8, val_ratio = 0.1):\n",
        "    \"\"\"Splits data into train, val, and test sets based on speaker. When\n",
        "    evaluating methods on the test split, we measure how well they generalize\n",
        "    to new (unseen) speakers.\n",
        "\n",
        "    Concretely, this stores and returns indices belonging to each split.\n",
        "    \"\"\"\n",
        "    # Fix seed so everyone reproduces the same splits.\n",
        "    rs = np.random.RandomState(42)\n",
        "\n",
        "    speaker_ids = self.label_data['speaker_ids']\n",
        "    unique_speaker_ids = sorted(list(set(speaker_ids)))\n",
        "    unique_speaker_ids = np.array(unique_speaker_ids)\n",
        "\n",
        "    # Shuffle so the speaker IDs are distributed.\n",
        "    rs.shuffle(unique_speaker_ids)\n",
        "\n",
        "    num_speaker = len(unique_speaker_ids)\n",
        "    num_train = int(train_ratio * num_speaker)\n",
        "    num_val = int(val_ratio * num_speaker)\n",
        "    num_test = num_speaker - num_train - num_val\n",
        "\n",
        "    train_speaker_ids = unique_speaker_ids[:num_train]\n",
        "    val_speaker_ids = unique_speaker_ids[num_train:num_train+num_val]\n",
        "    test_speaker_ids = unique_speaker_ids[num_train+num_val:]\n",
        "\n",
        "    train_speaker_dict = dict(zip(train_speaker_ids, ['train'] * num_train))\n",
        "    val_speaker_dict = dict(zip(val_speaker_ids, ['val'] * num_val))\n",
        "    test_speaker_dict = dict(zip(test_speaker_ids, ['test'] * num_test))\n",
        "    speaker_dict = {**train_speaker_dict, **val_speaker_dict,\n",
        "                    **test_speaker_dict}\n",
        "\n",
        "    train_indices, val_indices, test_indices = [], [], []\n",
        "    for i in range(len(speaker_ids)):\n",
        "      speaker_id = speaker_ids[i]\n",
        "      if speaker_dict[speaker_id] == 'train':\n",
        "          train_indices.append(i)\n",
        "      elif speaker_dict[speaker_id] == 'val':\n",
        "          val_indices.append(i)\n",
        "      elif speaker_dict[speaker_id] == 'test':\n",
        "          test_indices.append(i)\n",
        "      else:\n",
        "          raise Exception('split not recognized.')\n",
        "\n",
        "    train_indices = np.array(train_indices)\n",
        "    val_indices = np.array(val_indices)\n",
        "    test_indices = np.array(test_indices)\n",
        "\n",
        "    # Make sure to only keep \"valid indices\" i.e. those with more than 4\n",
        "    # words in the transcription.\n",
        "    train_indices = np.intersect1d(train_indices, valid_indices)\n",
        "    val_indices = np.intersect1d(val_indices, valid_indices)\n",
        "    test_indices = np.intersect1d(test_indices, valid_indices)\n",
        "\n",
        "    return train_indices, val_indices, test_indices\n",
        "\n",
        "  def get_primary_task_data(self, index):\n",
        "    \"\"\"Returns audio and transcript information for a single utterance.\n",
        "\n",
        "    Args:\n",
        "      index: Index of an utterance.\n",
        "\n",
        "    Returns:\n",
        "      log melspectrogram, wav length, transcript label, transcript length\n",
        "    \"\"\"\n",
        "    input_feature = None\n",
        "    input_length = None\n",
        "    human_transcript_label = None\n",
        "    human_transcript_length = None\n",
        "\n",
        "    wav = self.waveform_data[f'{index}'][:] # An h5py file uses string keys.\n",
        "    sr = 8000 # We fix the sample rate for you.\n",
        "\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(1.1)\n",
        "    # - Compute the mel spectrogram of the audio crop.\n",
        "    # - Convert the mel spectrogram to log space and normalize it.\n",
        "    # - This is your primary task feature. Note that models will expect feature\n",
        "    #   inputs of shape (T, n_mels).\n",
        "    # - Pad the feature so that all features are fixed-length and\n",
        "    #   convert it into a tensor.\n",
        "    # - Likewise, retrieve and pad the corresponding transcript label sequence.\n",
        "    #\n",
        "    # Hint:\n",
        "    # - Refer to https://librosa.org/doc/latest/index.html.\n",
        "    # - Use `librosa.feature.melspectrogram` and `librosa.util.normalize`.\n",
        "    # - Make sure to use our provided sr, n_mels, n_fft, win_length,\n",
        "    # - and hop_length\n",
        "    # - utils.py has helpful padding functions.\n",
        "\n",
        "\n",
        "    # Compute mel spectrogram\n",
        "    wav_mel = librosa.feature.melspectrogram(\n",
        "        y = wav,\n",
        "        sr = sr,\n",
        "        n_mels = self.n_mels,\n",
        "        n_fft = self.n_fft,\n",
        "        win_length= self.win_length,\n",
        "        hop_length=self.hop_length\n",
        "        )\n",
        "\n",
        "    # Convert to log scale and normalize\n",
        "    wav_mel_log = librosa.power_to_db(wav_mel)\n",
        "    wav_mel_log_normalized = librosa.util.normalize(wav_mel_log)\n",
        "\n",
        "    # Transpose to get (T, n_mels) shape\n",
        "    input_feature = wav_mel_log_normalized.transpose(1,0)\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    input_feature = torch.tensor(input_feature, dtype=torch.float32)\n",
        "\n",
        "    # Calculate input length (before padding)\n",
        "    original_length = input_feature.size(0)\n",
        "\n",
        "    if original_length > self.wav_max_length:\n",
        "      input_feature = input_feature[:self.wav_max_length]\n",
        "      input_length = self.wav_max_length\n",
        "    else:\n",
        "      padding_shape = (self.wav_max_length - original_length, self.n_mels)\n",
        "      padding = torch.zeros(padding_shape, dtype=torch.float32)\n",
        "      input_feature = torch.cat([input_feature, padding], dim=0)\n",
        "      input_length = original_length  # Keep the original length for CTC loss\n",
        "\n",
        "    # Get transcript label from the pre-processed labels\n",
        "    human_transcript_label = self.human_transcript_labels[index]\n",
        "\n",
        "    # Store the original transcript length before padding\n",
        "    human_transcript_length = min(len(human_transcript_label), self.transcript_max_length)\n",
        "\n",
        "    # padding the transcript labels\n",
        "    human_transcript_label, _ = pad_transcript_label(\n",
        "        human_transcript_label,\n",
        "        self.transcript_max_length,\n",
        "        pad = self.pad_index)\n",
        "\n",
        "    # Convert the padded label list to a PyTorch tensor\n",
        "    human_transcript_label = torch.tensor(human_transcript_label, dtype=torch.long)\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "\n",
        "    return input_feature, input_length, human_transcript_label, human_transcript_length\n",
        "\n",
        "  def load_waveforms(self):\n",
        "    # Make a file pointer to waveforms file.\n",
        "    waveform_h5 = h5py.File(os.path.join(self.root, 'data.h5'), 'r')\n",
        "    self.waveform_data = waveform_h5.get('waveforms')\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"Serves primary task data for a single utterance.\"\"\"\n",
        "    if not hasattr(self, 'waveform_data'):\n",
        "      # Do this in __getitem__ function so we enable multiprocessing.\n",
        "      self.load_waveforms()\n",
        "    index = int(self.indices[index])\n",
        "    return self.get_primary_task_data(index)\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Returns total number of utterances in the dataset.\"\"\"\n",
        "    return len(self.indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsxCZGiucJRc"
      },
      "source": [
        "**Sanity check.** Let's check that your dataset implementation is correct. This will be important to properly run our experiments in later parts. In particular, make sure your `__getitem__` and `__len__` are implemented correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTi-avLRcIuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb1c6b2-5f55-4194-eff3-b0d36dbd8b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Constructing HarperValleyBank train dataset...\n",
            "> Constructing HarperValleyBank val dataset...\n",
            "> Constructing HarperValleyBank test dataset...\n",
            "\n",
            "Validated dataset class implementation!\n"
          ]
        }
      ],
      "source": [
        " # Do not modify.\n",
        "root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
        "train_dataset = HarperValleyBank(root, split='train')\n",
        "val_dataset = HarperValleyBank(root, split='val')\n",
        "test_dataset = HarperValleyBank(root, split='test')\n",
        "\n",
        "assert len(train_dataset) == 10402\n",
        "assert len(val_dataset) == 679\n",
        "assert len(test_dataset) == 2854\n",
        "\n",
        "input, input_length, label, label_length = train_dataset.__getitem__(224)\n",
        "assert input.size() == torch.Size([train_dataset.wav_max_length, train_dataset.n_mels])\n",
        "assert input_length == 92\n",
        "assert label_length == 26\n",
        "print('\\nValidated dataset class implementation!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrjMYi5U6RkM"
      },
      "source": [
        "# Part 2: Connectionist Temporal Classification (CTC) Neural Network\n",
        "\n",
        "Our first experiment will be a [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) (Graves et al.) model trained on our primary task of speech recognition.\n",
        "\n",
        "As an overview, given an input matrix of shape `batch_size x sequence_length x feature_dim`, the network encodes the input speech features with an LSTM, producing a tensor of shape `batch_size x sequence_length x hidden_dim`. Using an additional linear layer, we transform this to `batch_size x sequence_length x vocab_size`, representing the probability of transcribing each character in the vocabulary at each time step. This is directly given to the CTC loss function.\n",
        "\n",
        "We will use [Weights & Biases](https://wandb.ai) to log loss curves and character error rates (CER) in the cloud. You can create a free account [here](https://wandb.ai/site)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMBltzWIRWRI"
      },
      "source": [
        "## **CTC Network**\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "You will use the CTC objective to train your network. Previously, you implemented the CTC loss function from scratch. For this assignment, you may use PyTorch's implementation. Filling out this section will be necessary to carry out later experiments.\n",
        "\n",
        "**→ Fill out `get_ctc_loss` using `F.ctc_loss`.**\n",
        "\n",
        "**→ Read through the starter code and fill out the `forward` pass of `CTCEncoderDecoder`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua4VUy5fwlEZ"
      },
      "outputs": [],
      "source": [
        "def get_ctc_loss(\n",
        "    log_probs, targets, input_lengths, target_lengths, blank=0):\n",
        "  \"\"\"Connectionist Temporal Classification objective function.\"\"\"\n",
        "  ctc_loss = None\n",
        "  log_probs = log_probs.contiguous()\n",
        "  targets = targets.long()\n",
        "  input_lengths = input_lengths.long()\n",
        "  target_lengths = target_lengths.long()\n",
        "  ############################ START OF YOUR CODE ############################\n",
        "  # TODO(2.1)\n",
        "  # Hint:\n",
        "  # - `F.ctc_loss`: https://pytorch.org/docs/stable/nn.functional.html#ctc-loss\n",
        "  # - log_probs is passed in with shape (batch_size, input_length, num_classes).\n",
        "  # - Notice that `F.ctc_loss` expects log_probs of shape\n",
        "  #   (input_length, batch_size, num_classes)\n",
        "  # - Turn on zero_infinity.\n",
        "\n",
        "  # reshaping log_probs to meet input shape for F.ctc_loss\n",
        "  log_probs_reshaped = log_probs.permute(1,0,2)\n",
        "\n",
        "  ctc_loss = F.ctc_loss(log_probs_reshaped, targets, input_lengths, target_lengths, blank = blank, zero_infinity=True  )\n",
        "\n",
        "  ############################# END OF YOUR CODE #############################\n",
        "  return ctc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb6IkAvR5xkY"
      },
      "outputs": [],
      "source": [
        "# this declares the CTC network architecture\n",
        "class CTCEncoderDecoder(nn.Module):\n",
        "  \"\"\"\n",
        "  Encoder-Decoder model trained with CTC objective.\n",
        "\n",
        "  Args:\n",
        "    input_dim: integer\n",
        "                number of input features\n",
        "    num_class: integer\n",
        "                size of transcription vocabulary\n",
        "    num_layers: integer (default: 2)\n",
        "                number of layers in encoder LSTM\n",
        "    hidden_dim: integer (default: 128)\n",
        "                number of hidden dimensions for encoder LSTM\n",
        "    bidirectional: boolean (default: True)\n",
        "                    is the encoder LSTM bidirectional?\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self, input_dim, num_class, num_layers=2, hidden_dim=128,\n",
        "      bidirectional=True):\n",
        "    super().__init__()\n",
        "    # Note: `batch_first=True` argument implies the inputs to the LSTM should\n",
        "    # be of shape (batch_size x T x D) instead of (T x batch_size x D).\n",
        "    self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
        "                            bidirectional=bidirectional, batch_first=True)\n",
        "    self.decoder = nn.Linear(hidden_dim * 2, num_class)\n",
        "    self.input_dim = input_dim\n",
        "    self.num_class = num_class\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = hidden_dim * num_layers * 2 * \\\n",
        "                          (2 if bidirectional else 1)\n",
        "\n",
        "  def combine_h_and_c(self, h, c):\n",
        "    \"\"\"Combine the signals from RNN hidden and cell states.\"\"\"\n",
        "    batch_size = h.size(1)\n",
        "    h = h.permute(1, 0, 2).contiguous()\n",
        "    c = c.permute(1, 0, 2).contiguous()\n",
        "    h = h.view(batch_size, -1)\n",
        "    c = c.view(batch_size, -1)\n",
        "    return torch.cat([h, c], dim=1)  # just concatenate\n",
        "\n",
        "  def forward(self, inputs, input_lengths):\n",
        "    batch_size, max_length, _ = inputs.size()\n",
        "    # `torch.nn.utils.rnn.pack_padded_sequence` collapses padded sequences\n",
        "    # to a contiguous chunk\n",
        "    inputs = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "        inputs, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    log_probs = None\n",
        "    h, c = None, None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(2.1)\n",
        "    # Hint:\n",
        "    # - Refer to https://pytorch.org/docs/stable/nn.html\n",
        "    # - Use `self.encoder` to get the encodings output which is of shape\n",
        "    #   (batch_size, max_length, num_directions*hidden_dim) and the\n",
        "    #   hidden states and cell states which are both of shape\n",
        "    #   (batch_size, num_layers*num_directions, hidden_dim)\n",
        "    # - Pad outputs with `0.` using `torch.nn.utils.rnn.pad_packed_sequence`\n",
        "    #   (turn on batch_first and set total_length as max_length).\n",
        "    # - Apply 50% dropout.\n",
        "    # - Use `self.decoder` to take the embeddings sequence and return\n",
        "    #   probabilities for each character.\n",
        "    # - Make sure to then convert to log probabilities.\n",
        "\n",
        "    # Pass packed sequence through the encoder\n",
        "    # Note: LSTM returns (output, (h, c))\n",
        "    output, (h, c) = self.encoder(inputs)\n",
        "\n",
        "\n",
        "    # Unpack the packed sequence\n",
        "    # pad_packed_sequence returns (padded_seq, original_lengths)\n",
        "    encoding_padded, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
        "        output,\n",
        "        total_length = max_length,\n",
        "        batch_first = True\n",
        "        )\n",
        "\n",
        "    # Apply dropout to the padded output\n",
        "    dropout = nn.Dropout(0.5)\n",
        "    x = dropout(encoding_padded)\n",
        "\n",
        "    logits = self.decoder(x)\n",
        "\n",
        "    # Apply log_softmax to get log probabilities\n",
        "    log_probs = F.log_softmax(logits, dim=2)\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "\n",
        "    # The extracted embedding is not used for the ASR task but will be\n",
        "    # needed for other auxiliary tasks.\n",
        "    embedding = self.combine_h_and_c(h, c)\n",
        "    return log_probs, embedding\n",
        "\n",
        "  def get_loss(\n",
        "      self, log_probs, targets, input_lengths, target_lengths, blank=0):\n",
        "    return get_ctc_loss(\n",
        "        log_probs, targets, input_lengths, target_lengths, blank)\n",
        "\n",
        "  def decode(self, log_probs, input_lengths, labels, label_lengths,\n",
        "             sos_index, eos_index, pad_index, eps_index):\n",
        "    # Use greedy decoding.\n",
        "    decoded = torch.argmax(log_probs, dim=2)\n",
        "    batch_size = decoded.size(0)\n",
        "    # Collapse each decoded sequence using CTC rules.\n",
        "    hypotheses = []\n",
        "    for i in range(batch_size):\n",
        "      hypotheses_i = self.ctc_collapse(decoded[i], input_lengths[i].item(),\n",
        "                                       blank_index=eps_index)\n",
        "      hypotheses.append(hypotheses_i)\n",
        "\n",
        "    hypothesis_lengths = input_lengths.cpu().numpy().tolist()\n",
        "    if labels is None: # Run at inference time.\n",
        "      references, reference_lengths = None, None\n",
        "    else:\n",
        "      references = labels.cpu().numpy().tolist()\n",
        "      reference_lengths = label_lengths.cpu().numpy().tolist()\n",
        "\n",
        "    return hypotheses, hypothesis_lengths, references, reference_lengths\n",
        "\n",
        "  def ctc_collapse(self, seq, seq_len, blank_index=0):\n",
        "    result = []\n",
        "    for i, tok in enumerate(seq[:seq_len]):\n",
        "      if tok.item() != blank_index:  # remove blanks\n",
        "        if i != 0 and tok.item() == seq[i-1].item():  # remove dups\n",
        "          pass\n",
        "        else:\n",
        "          result.append(tok.item())\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nnZCcWpLTzm"
      },
      "source": [
        "## **Introduction to PyTorch Lightning**\n",
        "\n",
        "**Walkthrough**\n",
        "\n",
        "*This section is a walkthrough and will not require any code or answers.* We will use [PyTorch Lightning](https://www.pytorchlightning.ai/), a lightweight wrapper framework for PyTorch, to run our experiments. You can learn more about the lightning toolkit [here](https://github.com/PyTorchLightning/pytorch-lightning). As a short introduction, Pytorch Lightning is a scaffold for training deep learning models. It handles a lot of the usual pipeline for you (e.g. looping over the training set, calling your optimizer). It has several callback handlers you can overwrite to specify your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fD95O2MLRWN"
      },
      "outputs": [],
      "source": [
        "# Do not modify.\n",
        "\n",
        "class LightningCTC(pl.LightningModule):\n",
        "  \"\"\"PyTorch Lightning class for training a CTC model.\n",
        "\n",
        "  Args:\n",
        "    n_mels: number of mel frequencies. (default: 128)\n",
        "    n_fft: number of fourier features. (default: 256)\n",
        "    win_length: number of frames in a window. (default: 256)\n",
        "    hop_length: number of frames to hop in computing spectrogram. (default: 128)\n",
        "    wav_max_length: max number of timesteps in a waveform spectrogram. (default: 200)\n",
        "    transcript_max_length: max number of characters in decoded transcription. (default: 200)\n",
        "    learning_rate: learning rate for Adam optimizer. (default: 1e-3)\n",
        "    batch_size: batch size used in optimization and evaluation. (default: 256)\n",
        "    weight_decay: weight decay for Adam optimizer. (default: 1e-5)\n",
        "    encoder_num_layers: number of layers in LSTM encoder. (default: 2)\n",
        "    encoder_hidden_dim: number of hidden dimensions in LSTM encoder. (default: 256)\n",
        "    encoder_bidirectional: directionality of LSTM encoder. (default: True)\n",
        "  \"\"\"\n",
        "  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128,\n",
        "               wav_max_length=200, transcript_max_length=200,\n",
        "               learning_rate=1e-3, batch_size=256, weight_decay=1e-5,\n",
        "               encoder_num_layers=2, encoder_hidden_dim=256,\n",
        "               encoder_bidirectional=True):\n",
        "    super().__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.n_mels = n_mels\n",
        "    self.n_fft = n_fft\n",
        "    self.win_length = win_length\n",
        "    self.hop_length = hop_length\n",
        "    self.lr = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.weight_decay = weight_decay\n",
        "    self.wav_max_length = wav_max_length\n",
        "    self.transcript_max_length = transcript_max_length\n",
        "    self.train_dataset, self.val_dataset, self.test_dataset = \\\n",
        "      self.create_datasets()\n",
        "    self.encoder_num_layers = encoder_num_layers\n",
        "    self.encoder_hidden_dim = encoder_hidden_dim\n",
        "    self.encoder_bidirectional = encoder_bidirectional\n",
        "\n",
        "    # Instantiate the CTC encoder/decoder.\n",
        "    self.model = self.create_model()\n",
        "\n",
        "  def create_model(self):\n",
        "    model = CTCEncoderDecoder(\n",
        "      self.train_dataset.input_dim,\n",
        "      self.train_dataset.num_class,\n",
        "      num_layers=self.encoder_num_layers,\n",
        "      hidden_dim=self.encoder_hidden_dim,\n",
        "      bidirectional=self.encoder_bidirectional)\n",
        "    return model\n",
        "\n",
        "  def create_datasets(self):\n",
        "    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
        "    train_dataset = HarperValleyBank(\n",
        "        root, split='train', n_mels=self.n_mels, n_fft=self.n_fft,\n",
        "        win_length=self.win_length, hop_length=self.hop_length,\n",
        "        wav_max_length=self.wav_max_length,\n",
        "        transcript_max_length=self.transcript_max_length,\n",
        "        append_eos_token=False)\n",
        "    val_dataset = HarperValleyBank(\n",
        "        root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n",
        "        win_length=self.win_length, hop_length=self.hop_length,\n",
        "        wav_max_length=self.wav_max_length,\n",
        "        transcript_max_length=self.transcript_max_length,\n",
        "        append_eos_token=False)\n",
        "    test_dataset = HarperValleyBank(\n",
        "        root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n",
        "        win_length=self.win_length, hop_length=self.hop_length,\n",
        "        wav_max_length=self.wav_max_length,\n",
        "        transcript_max_length=self.transcript_max_length,\n",
        "        append_eos_token=False)\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optim = torch.optim.AdamW(self.model.parameters(),\n",
        "                              lr=self.lr, weight_decay=self.weight_decay)\n",
        "    return [optim], [] # <-- put scheduler in here if you want to use one\n",
        "\n",
        "  def get_loss(self, log_probs, input_lengths, labels, label_lengths):\n",
        "    loss = self.model.get_loss(log_probs, labels, input_lengths, label_lengths,\n",
        "                                blank=self.train_dataset.eps_index)\n",
        "    return loss\n",
        "\n",
        "  def forward(self, inputs, input_lengths, labels, label_lengths):\n",
        "    log_probs, embedding = self.model(inputs, input_lengths)\n",
        "    return log_probs, embedding\n",
        "\n",
        "  def get_primary_task_loss(self, batch, split='train'):\n",
        "    \"\"\"Returns ASR model losses, metrics, and embeddings for a batch.\"\"\"\n",
        "    inputs, input_lengths = batch[0], batch[1]\n",
        "    labels, label_lengths = batch[2], batch[3]\n",
        "\n",
        "    if split == 'train':\n",
        "      log_probs, embedding = self.forward(\n",
        "          inputs, input_lengths, labels, label_lengths)\n",
        "    else:\n",
        "      # do not pass labels to not teacher force after training\n",
        "      log_probs, embedding = self.forward(\n",
        "          inputs, input_lengths, None, None)\n",
        "\n",
        "    loss = self.get_loss(log_probs, input_lengths, labels, label_lengths)\n",
        "\n",
        "    # Compute CER (no gradient necessary).\n",
        "    with torch.no_grad():\n",
        "      hypotheses, hypothesis_lengths, references, reference_lengths = \\\n",
        "        self.model.decode(\n",
        "            log_probs, input_lengths, labels, label_lengths,\n",
        "            self.train_dataset.sos_index,\n",
        "            self.train_dataset.eos_index,\n",
        "            self.train_dataset.pad_index,\n",
        "            self.train_dataset.eps_index)\n",
        "      cer_per_sample = get_cer_per_sample(\n",
        "          hypotheses, hypothesis_lengths, references, reference_lengths)\n",
        "      cer = cer_per_sample.mean()\n",
        "      metrics = {f'{split}_loss': loss, f'{split}_cer': cer}\n",
        "\n",
        "    return loss, metrics, embedding\n",
        "\n",
        "  # Overwrite TRAIN\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    loss, metrics, _ = self.get_primary_task_loss(batch, split='train')\n",
        "    self.log_dict(metrics)\n",
        "    # self.log('train_loss', loss, prog_bar=True, on_step=True)\n",
        "    # self.log('train_cer', metrics['train_cer'], prog_bar=True, on_step=True)\n",
        "    return loss\n",
        "\n",
        "  # Overwrite VALIDATION: get next minibatch\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    loss, metrics, _ = self.get_primary_task_loss(batch, split='val')\n",
        "    self.log(\"val_loss\", metrics[\"val_loss\"],\n",
        "             prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "    self.log(\"val_cer\",  metrics[\"val_cer\"],\n",
        "             prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "    return metrics\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    loss, metrics, _ = self.get_primary_task_loss(batch, split='test')\n",
        "    self.log(\"test_loss\", metrics[\"test_loss\"],\n",
        "             prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "    self.log(\"test_cer\",  metrics[\"test_cer\"],\n",
        "             prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "    return metrics\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    # - important to shuffle to not overfit!\n",
        "    # - drop the last batch to preserve consistent batch sizes\n",
        "    loader = DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
        "                        shuffle=True, pin_memory=True, drop_last=True)\n",
        "    return loader\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    loader = DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
        "                        shuffle=False, pin_memory=True)\n",
        "    return loader\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    loader = DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
        "                        shuffle=False, pin_memory=True)\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY6Sl-rH950z"
      },
      "source": [
        "## **Task 2.1: Train a network with CTC [20 Points]**\n",
        "\n",
        "**Training & Written Response**\n",
        "\n",
        "Go to **Runtime** > **Change runtime type** and set **Hardware accelerator** to **GPU**.\n",
        "\n",
        "This section will be graded based on 1) your model's performance in regards to loss plots and CER plots and 2) your response for qualitative assessments of your plots.\n",
        "\n",
        "**→ Train the CTC network with the default hyperparameters we provide.**\n",
        "\n",
        "With batch size 128, one epoch of optimizing CTC takes roughly 3 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n",
        "\n",
        "**CER target:\n",
        "You should obtain a test CER of at most 0.35 for this model. You will obtain full points for demonstrating a model with test CER below this threshold.**\n",
        "\n",
        "**→ Paste screenshots from your Weights & Biases dashboard of your loss curve and CER curve in the cell marked \"Plots\".**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viZWdpyhJkX0"
      },
      "outputs": [],
      "source": [
        "WANDB_NAME = 'shubham13596-self' # Fill in your Weights & Biases ID here.\n",
        "#api_key = 8a5377984a305230e33e41a164a5d23198c414c6\n",
        "\n",
        "def run(system, config, ckpt_dir, epochs=1, monitor_key='val_loss',\n",
        "        use_gpu=False, seed=1337):\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "  SystemClass = globals()[system]\n",
        "  system = SystemClass(**config)\n",
        "\n",
        "  checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=os.path.join(MODEL_PATH, ckpt_dir),\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=monitor_key,\n",
        "    mode='min')\n",
        "\n",
        "  wandb.init(project='cs224s', entity=WANDB_NAME, name=ckpt_dir,\n",
        "             config=config, sync_tensorboard=True)\n",
        "  wandb_logger = WandbLogger()\n",
        "\n",
        "  if use_gpu:\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "      accelerator=\"gpu\",\n",
        "      devices=1,\n",
        "      max_epochs=epochs,\n",
        "      min_epochs=epochs,\n",
        "      enable_checkpointing=True,\n",
        "      callbacks=[checkpoint_callback],\n",
        "      logger=wandb_logger\n",
        "    )\n",
        "  else:\n",
        "    trainer = pl.Trainer(\n",
        "      accelerator=\"cpu\",\n",
        "      max_epochs=epochs,\n",
        "      min_epochs=epochs,\n",
        "      enable_checkpointing=True,\n",
        "      callbacks=[checkpoint_callback],  # Note: must be a list now\n",
        "      logger=wandb_logger\n",
        "    )\n",
        "\n",
        "  trainer.fit(system)\n",
        "  result = trainer.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H9tt8gSKVH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5967f166116f42e9a994d27bb3b8f0d7",
            "530208248c8f4b77b1935893c3a4d9f9",
            "2cd94efbaae54fa6b0377f42fd5d6fdb",
            "517825ac83844f7083ee23b0406745ba",
            "caa2b2fd48e64e1a8b1cfa9bdc0f2758",
            "ec6f70996d4a4cdfa056d01cc701f72d",
            "4caadfbb66194d1d8106cf3b06ff1579",
            "2ea65c0f0e5f45dbb74a43fffd7176a5",
            "7c204b2e5661406380d089b922f86337",
            "b147b8ffbf174a75a8bb5ff307ad9c46",
            "f1a22e859bbe4dd68ceb141682af819f",
            "9a63d23beaa8468dbac47676dd8d7969",
            "cbbbb0786ff54c97b1548c4eb72fed3f",
            "f5770451a9724cf4a4619121c1acef1e",
            "8a1e10226f4845d6bd8018259bf26369",
            "6f55e7398a114cb8ae926e1a44cd687a",
            "907be565f3e841b789962ca3aa6b518d",
            "ad2fdaa32569432fac39a68593cf49e4",
            "a51254190d3f4adba517799a6ab6892e",
            "79942ec59935415da9c4d44a806f3c51",
            "d96ff8af941a4a88850a365ad9a1086d",
            "40e5618ab4b64240acaf5691e9fe41c5",
            "b24813aa9dea4b249fab082079c646b0",
            "88710e47bbee440b93a74a9797df173c",
            "83d0c4d326484c34ae4eb9817a906f16",
            "0ebee3fbaa7645bbb2c0a16c0ca49bb9",
            "a9c6a2156bef47f290b55d80e88cd4e6",
            "472d93a5969e42999f3b9a1bfb585f70",
            "521e68cecfdf4e6286d3814d9dd2946c",
            "a86d8707b4fa486bbfb7280b1fdfa30d",
            "fe006e87254e44acba2b3613d2d13124",
            "334e21bd1b094c9a8a25d85e1af99cb4",
            "6a21d49482b64c9580a2055eab42f97e",
            "fd1c2f28c8a048f2aad1f5b8b824f411",
            "0c9d1ae9cab6439e8179020226a9ded5",
            "f4c348beffed4a8980a6d39e73be9678",
            "94ee98624a3b49e39a3313520c3f9716",
            "25f10fc30b7c4415af81db86b74e2e1b",
            "6e39672ead1c4257abbdd55b879974cc",
            "3ac6bb040261450fb40d9f8c03b13aed",
            "3c609f8239e141eabb4d67b3fedf1557",
            "bc0ceec353114203bfcd2ffa77217323",
            "177e23603dd8494486ac4c70ddc1ff0c",
            "3769bc338a694ad2afccb6fecb11b0ac",
            "9ba87da05b7648328aa23d54d8ffe2c5",
            "5f31a073cd9747f384cbdddcdd2ce3e3",
            "61e094acc75043c9975fecafed0c17bb",
            "88d45b4f89e64642ab487c6f4161add1",
            "afd8371e886741c8aae051ca41aafbbb",
            "bb3373dfc20f4dddad3ab7b88cc9bef1",
            "177d5b681e9b467b80fd64979eb943b6",
            "ae840da533c04ea28c1ce449d6e8a5b5",
            "826d67a85061401aa985bf020f41ae2d",
            "a1e483f138b5429cb899d6e779b1a48d",
            "d9a09833f8e5421ea8e604f7e5260c46",
            "9b9e9ce86ca24d9face6e77eac0f1c32",
            "737c96bba79a4d2b8b9170bc1f895604",
            "034beff7e1eb4d9fbaedd03de184784b",
            "5c47fcf33d1441678729a670dc8bc93f",
            "a103b699179f4684959f40a5485481b7",
            "7bd144b00eb44e7096905e304232660e",
            "76f5b9965b6e4337a6ae02c066d73875",
            "09f7c795d13e498c82e94162829d5132",
            "54c288d8b8b04b4192241944c7a91ab4",
            "2a48b7868efd44d7a5dadc407e94a336",
            "d465b33b8dcc4e34a35883dc11bcb531",
            "1192c0cb494d4626a7c03ee96b20055d",
            "fe46499e0b2f4ca3b145feda2499298b",
            "c10cf586d51c43a78a3e41320c925080",
            "6f2d436367f94bf99bbe036409efb9bf",
            "f048eccb3e2b4dacbe23ed655c3faa67",
            "4588a4da2aa74bbfaf81b10ed7420087",
            "0de4952b02cd47e9a3b01fb31bca3c34",
            "90a61b38d5234b42ba8a7b50809d555c",
            "48caf4f3e01b4b488ced6e6a6644648e",
            "d95e2b8becd548a6b33f983817651ca2",
            "e5a31279201543efba9c68cc8702b887"
          ]
        },
        "outputId": "62d870de-dc2d-4bf2-e0c4-03618c79cd23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Constructing HarperValleyBank train dataset...\n",
            "> Constructing HarperValleyBank val dataset...\n",
            "> Constructing HarperValleyBank test dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train_cer</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁█</td></tr><tr><td>val_cer</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_cer</td><td>1</td></tr><tr><td>train_loss</td><td>3.01625</td></tr><tr><td>trainer/global_step</td><td>80</td></tr><tr><td>val_cer</td><td>1</td></tr><tr><td>val_loss</td><td>3.01708</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ctc</strong> at: <a href='https://wandb.ai/shubham13596-self/cs224s/runs/ojw28hpp' target=\"_blank\">https://wandb.ai/shubham13596-self/cs224s/runs/ojw28hpp</a><br> View project at: <a href='https://wandb.ai/shubham13596-self/cs224s' target=\"_blank\">https://wandb.ai/shubham13596-self/cs224s</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250517_121906-ojw28hpp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/gdrive/MyDrive/cs224s_spring2025/wandb/run-20250517_122848-m9mx4vue</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shubham13596-self/cs224s/runs/m9mx4vue' target=\"_blank\">ctc</a></strong> to <a href='https://wandb.ai/shubham13596-self/cs224s' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/shubham13596-self/cs224s' target=\"_blank\">https://wandb.ai/shubham13596-self/cs224s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/shubham13596-self/cs224s/runs/m9mx4vue' target=\"_blank\">https://wandb.ai/shubham13596-self/cs224s/runs/m9mx4vue</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /content/gdrive/MyDrive/cs224s_spring2025/trained_models/ctc exists and is not empty.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type              | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | model | CTCEncoderDecoder | 2.4 M  | train\n",
            "----------------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.568     Total estimated model params size (MB)\n",
            "3         Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5967f166116f42e9a994d27bb3b8f0d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a63d23beaa8468dbac47676dd8d7969"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b24813aa9dea4b249fab082079c646b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 81: 'val_loss' reached 3.01708 (best 3.01708), saving model to '/content/gdrive/MyDrive/cs224s_spring2025/trained_models/ctc/epoch=0-step=81-v1.ckpt' as top 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd1c2f28c8a048f2aad1f5b8b824f411"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 162: 'val_loss' reached 3.00081 (best 3.00081), saving model to '/content/gdrive/MyDrive/cs224s_spring2025/trained_models/ctc/epoch=1-step=162.ckpt' as top 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ba87da05b7648328aa23d54d8ffe2c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 243: 'val_loss' reached 2.97444 (best 2.97444), saving model to '/content/gdrive/MyDrive/cs224s_spring2025/trained_models/ctc/epoch=2-step=243-v1.ckpt' as top 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b9e9ce86ca24d9face6e77eac0f1c32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 324: 'val_loss' reached 2.95476 (best 2.95476), saving model to '/content/gdrive/MyDrive/cs224s_spring2025/trained_models/ctc/epoch=3-step=324.ckpt' as top 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1192c0cb494d4626a7c03ee96b20055d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 405: 'val_loss' reached 2.93068 (best 2.93068), saving model to '/content/gdrive/MyDrive/cs224s_spring2025/trained_models/ctc/epoch=4-step=405.ckpt' as top 1\n",
            "INFO:pytorch_lightning.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    598\u001b[0m         )\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0;31m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# this will run only when no pre-fetching was done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Sequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-61a9c7a318b3>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_primary_task_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-61a9c7a318b3>\u001b[0m in \u001b[0;36mget_primary_task_data\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m       \u001b[0minput_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m       \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_length\u001b[0m  \u001b[0;31m# Keep the original length for CTC loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1396abef4055>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# - Every validation loop, the best performing model is saved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# - After training, the system will evaluate performance on the test set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LightningCTC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ctc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-cdf48935ed8e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(system, config, ckpt_dir, epochs, monitor_key, use_gpu, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    'n_mels': 128,\n",
        "    'n_fft': 256,\n",
        "    'win_length': 256,\n",
        "    'wav_max_length': 512,\n",
        "    'hop_length': 128,\n",
        "    'transcript_max_length': 200,\n",
        "    'learning_rate': 1e-3,\n",
        "    'batch_size': 128,\n",
        "    'weight_decay': 0,\n",
        "    'encoder_num_layers': 2,\n",
        "    'encoder_hidden_dim': 256,\n",
        "    'encoder_bidirectional': True,\n",
        "}\n",
        "\n",
        "# NOTES:\n",
        "# -----\n",
        "# - PyTorch Lightning will run 2 steps of validation prior to the first\n",
        "#   epoch to sanity check that validation works (otherwise you\n",
        "#   might waste an epoch training and error).\n",
        "# - The progress bar updates very slowly, the model is likely\n",
        "#   training even if it doesn't look like it is.\n",
        "# - Wandb will generate a URL for you where all the metrics will be logged.\n",
        "# - Every validation loop, the best performing model is saved.\n",
        "# - After training, the system will evaluate performance on the test set.\n",
        "run(system=\"LightningCTC\", config=config, ckpt_dir='ctc', epochs=20, use_gpu=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "812MYw9LCvWM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgZmM-cKtpaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da46efb-7789-4202-9706-264e50a7d92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'epoch=0-step=81.ckpt'\t'epoch=2-step=243.ckpt'  'epoch=4-step=405.ckpt'\n"
          ]
        }
      ],
      "source": [
        "# You can find the saved checkpoint here:\n",
        "!ls /content/cs224s_spring2025/trained_models/ctc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvgmMqA7crtI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "8d7d5eaa-b641-43fb-aa35-6c9979c2e39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Constructing HarperValleyBank train dataset...\n",
            "> Constructing HarperValleyBank val dataset...\n",
            "> Constructing HarperValleyBank test dataset...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LightningCTC(\n",
              "  (model): CTCEncoderDecoder(\n",
              "    (encoder): LSTM(128, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
              "    (decoder): Linear(in_features=512, out_features=48, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ctc_checkpoint_file ='epoch=4-step=405.ckpt' # Fill in your checkpoint file\n",
        "ctc_checkpoint_path = os.path.join(MODEL_PATH, 'ctc', ctc_checkpoint_file)\n",
        "\n",
        "LightningCTC.load_from_checkpoint(ctc_checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_zuwGLr0Pi7"
      },
      "source": [
        "---\n",
        "\n",
        "**Plots:**\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IXjPxEYMOUB"
      },
      "source": [
        "**→ Using your plots as evidence in your description, answer the following questions:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a) What is the model's best test CER?\n",
        "\n",
        "b) Does the model learn and converge? What do you notice about CTC loss early in training?\n",
        "\n",
        "c) Does the model overfit? Despite the small dataset size, why might CTC not overfit?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Fkkc9O1SYv"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:**\n",
        "(Your answer here)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngMF5nRLXLJ1"
      },
      "source": [
        "# Part 3: Analysis\n",
        "\n",
        "While looking at validation and test CER is a good way to judge how a model is performing, it is also important to look at specific examples it does well on or fails on, in order to build an intuition for why it fails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfYm97bGZpyZ"
      },
      "source": [
        "## **Task 3.1: Lowest and Highest CER Examples [5 Points]**\n",
        "\n",
        "**Implementation & Written Response**\n",
        "\n",
        "**→ Now we will find and examine a test utterance your model transcribes well and a test utterance it transcribes poorly.** Fill out `get_low_high_cer_wav` to get the lowest and highest CERs and their corresponding utterances in your test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrF1Mt6w1g55"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_low_high_cer_wav(system, device=None):\n",
        "  \"\"\"Gets the test set sample with lowest CER and the sample with highest CER.\n",
        "\n",
        "  Args:\n",
        "    system: Subclassed LightningModule for your model.\n",
        "    device: Instance of torch.device(...) [default: None]\n",
        "\n",
        "  Returns:\n",
        "    lowest CER (float), audio of the lowest CER utterance (ndarray),\n",
        "    highest CER (float), audio of the highest CER utterance (ndarray)\n",
        "  \"\"\"\n",
        "  # Init values.\n",
        "  low_cer = float('inf')\n",
        "  low_idx = 0\n",
        "  high_cer = float('-inf')\n",
        "  high_idx = 0\n",
        "\n",
        "  test_dataloader = system.test_dataloader()\n",
        "  index_lookup = system.test_dataset.indices\n",
        "\n",
        "  pbar = tqdm(total=len(test_dataloader))\n",
        "  for i, batch in enumerate(test_dataloader):\n",
        "    input_features, input_lengths = batch[0], batch[1]\n",
        "    labels, label_lengths = batch[2], batch[3]\n",
        "    batch_size = input_features.size(0)\n",
        "    if device is not None:\n",
        "      input_features = input_features.to(device)\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(3.1)\n",
        "    # Hint:\n",
        "    # - Use `get_cer_per_sample`, which gets a numpy array of\n",
        "    #   CERs for each sample in a batch\n",
        "    # - Use `index_lookup` to map a sample's test set index to\n",
        "    #   its index in the full dataset.\n",
        "\n",
        "    log_probs, _ = system.model(input_features, input_lengths)\n",
        "    hypotheses, hypothesis_lengths, references, reference_lengths = system.model.decode(\n",
        "        log_probs, input_lengths, labels, label_lengths,\n",
        "        system.test_dataset.sos_index, system.test_dataset.eos_index,\n",
        "        system.test_dataset.pad_index, system.test_dataset.eps_index)\n",
        "\n",
        "    cer_per_sample = get_cer_per_sample(hypotheses, hypothesis_lengths, references, reference_lengths)\n",
        "\n",
        "    for j in range(batch_size):\n",
        "      batch_idx = i*batch_size + j\n",
        "      if batch_idx >= len(index_lookup):\n",
        "        continue\n",
        "      # Get the original index in the full dataset\n",
        "      full_idx = int(index_lookup[batch_idx])\n",
        "\n",
        "      # Check if this sample has the lowest CER so far\n",
        "      if cer_per_sample[j] < low_cer:\n",
        "          low_cer = cer_per_sample[j]\n",
        "          low_idx = full_idx\n",
        "\n",
        "      # Check if this sample has the highest CER so far\n",
        "      if cer_per_sample[j] > high_cer:\n",
        "          high_cer = cer_per_sample[j]\n",
        "          high_idx = full_idx\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    pbar.update()\n",
        "  pbar.close()\n",
        "\n",
        "  # Retrieve ndarray wav data from the original h5py file.\n",
        "  system.test_dataset.load_waveforms()\n",
        "  waveform_data = system.test_dataset.waveform_data\n",
        "  low_wav = waveform_data[f'{low_idx}'][:]\n",
        "  high_wav = waveform_data[f'{high_idx}'][:]\n",
        "\n",
        "  return low_cer, low_wav, high_cer, high_wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie7HDMvVqgSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6629479e-c039-4055-96e4-b2c4aae91beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Constructing HarperValleyBank train dataset...\n",
            "> Constructing HarperValleyBank val dataset...\n",
            "> Constructing HarperValleyBank test dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|▍         | 1/23 [00:04<01:45,  4.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "  9%|▊         | 2/23 [00:09<01:40,  4.77s/it]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 3/23 [00:14<01:39,  4.98s/it]\u001b[A\u001b[A\n",
            "\n",
            " 17%|█▋        | 4/23 [00:19<01:30,  4.76s/it]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 5/23 [00:23<01:25,  4.74s/it]\u001b[A\u001b[A\n",
            "\n",
            " 26%|██▌       | 6/23 [00:28<01:20,  4.74s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 7/23 [00:33<01:17,  4.83s/it]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▍      | 8/23 [00:38<01:10,  4.70s/it]\u001b[A\u001b[A\n",
            "\n",
            " 39%|███▉      | 9/23 [00:42<01:04,  4.63s/it]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 10/23 [00:47<01:02,  4.82s/it]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 11/23 [00:51<00:55,  4.62s/it]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 12/23 [00:57<00:52,  4.77s/it]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 13/23 [01:02<00:48,  4.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 61%|██████    | 14/23 [01:05<00:40,  4.55s/it]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▌   | 15/23 [01:10<00:36,  4.54s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|██████▉   | 16/23 [01:15<00:33,  4.76s/it]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 17/23 [01:20<00:27,  4.61s/it]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 18/23 [01:24<00:22,  4.48s/it]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 19/23 [01:29<00:18,  4.69s/it]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 20/23 [01:34<00:14,  4.68s/it]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████▏| 21/23 [01:38<00:09,  4.68s/it]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 22/23 [01:44<00:04,  4.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 23/23 [01:45<00:00,  4.60s/it]\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = ctc_checkpoint_path\n",
        "\n",
        "device = torch.device('cuda')\n",
        "system = LightningCTC.load_from_checkpoint(checkpoint_path)\n",
        "system = system.to(device)\n",
        "system.eval()\n",
        "low_cer, low_wav, high_cer, high_wav = get_low_high_cer_wav(system, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTK_pU35Jl8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21686cd-506b-442f-be51-a37c5b27f8eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9285714285714286)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "low_cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdGCi9pXJoFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5cae4b-7557-4b37-f9d3-7ccfc1c6caba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "high_cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wygAmfkc-H5n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "46e1fab1-3ce1-43a4-a46e-e1418cce7911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utterance with lowest CER: 0.9285714285714286\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,UklGRqRDAABXQVZFZm10IBAAAAABAAEAQB8AAIA+AAACABAAZGF0YYBDAAAeAAoA2P+S/5z/uv/E/+L/9v8KAAAA9v/2/+z/4v/2/wAAHgAoACgAKAAoABQAFAAeADIAKAAoABQACgAAAPb/AAAAAAAA9v/2//b/7P/s/+z/7P/s/+z/7P/s/+z/7P/s/+z/7P/s/wAAAAD2/+z/7P/s/+z/4v/i/+z/7P/s/+z/AAAKAAoAFAAeABQACgD2/+z/4v/Y/9j/2P/i/+z/4v/i/+z/9v8AAAoAAAAAAAAAFAA8AFAARgAyACgAMgAoAB4AHgAoACgAFAAKAAAAAADi/9j/zv/O/87/zv/Y/+z/7P/s/wAAFAAUABQAHgAoACgAKAAeABQAHgAoADIAPAA8ACgAHgAKAPb/4v/Y/4f/Wv+S/7D/sP+6/9j/AAAAAPb/9v8AAAAACgAUABQACgAKAAAACgAAAAAAAAAAAAAA9v8AAM7/xP/Y/8T/zv/Y/9j/zv/O/+z/4v/2/woAFAAKAAAAFAAKAAoAFAAoACgAMgAyACgAPAA8ACgAMgA8ADwAKAAeABQAKAAeABQAFAAKAAAA9v/2/wAAAAAAABQAHgAeABQAFAAKABQAFAAKAAoAAAAKAAAA9v8KABQAFAAKAAAA4v/O/5z/Wv99/7D/uv/O/9j/2P/O/9j/2P/i/+L/7P/2/wAAAAAKAAoACgAKAAAACgAeAB4AHgAeABQAHgAeAB4AHgAUABQACgAAAAAAAAAAAAAA9v/2//b/9v8KABQACgAKAB4AMgA8ACgAKAAeABQACgAAAAAAAAD2//b/9v/2/+z/9v/2/wAACgDi/wAAAADs//b/9v/2/+z/7P/s/+z/9v/2/+z/4v/i/+L/4v/s/wAACgAeAB4AFAAUABQACgAAAAoACgAKAAAAAAAAAPb/2P/O/9j/uv/E/8T/zv/Y/+z/7P/s//b/AAAAAAAAFAAUABQAFAAUABQAFAAUABQAFAAUACgAKAAeAB4AAACc/5z/2P/i/wAAHgAoACgAHgAKAPb/7P8AAAAAFAAeAAoACgD2/+z/9v/2/woACgAUABQACgAKAAoAKAAoACgAKAAUABQACgAKAAAA7P/2/woAFAAKACgAMgAoAB4AKAAyACgAFAAKAAoACgAUABQAFAAAAOz/7P/i/wAACgAAAPb/9v8UAAAA2P/i/+L/7P/i/+z/7P/i/+L/4v/i/+z/4v/i/woA9v/s/+z/7P8UAOL/4v/i/+z/7P/Y/+z/7P/O/8T/nP+c/1r/Mv8d/zL/Mv/h/vX+9f4d/zL/Wv+S/87/HgAeADwAWgB5AI0ApgC6APcARwFwAZgB8wEbAvMB8wHzAdUBmAGEAUcBhAEbAr0C5QINA5QC8wHjAKb/aP5D/cr8Kfwp/Hn88/yU/Q3+uf7h/jL/Mv8y/1r/Mv9G/wn/Mv/h/pD+aP5U/nz+aP4J/8T/gwBwARsCvQLlApQCGwJcAY0AnP/N/j/+5f2U/UP9Q/1s/bz9Df5o/s3+Mv8d/zL/Cf+k/nz+Df4N/j/+Df4N/lT+9f7h/lr/zv+DAEcBwQGUAocD3gTCBvcIpQuJDcwOzA6JDQQLtAevA3P/Ufw++ar3uPa49qr3nfiB+tj7G/0r/uH+Cf9a/1r/Rv8y/+H+K/68/fP8Kfxz+3P70voi+3P7ovzl/Vr/jQCEAb0CNgM2A+UC5QKUAhsCRwELAc4ARgDN/pD+4f6k/mj+pP66/24A4wCsATYDKARQBCgEAAReA8EBUAB9/6T+bP2i/Mr88/wb/Wz9Df7N/gn/9f4d/1r/Cf/N/uH+Cf8J/x3/h//i/8T/4v9QADIAFAA8AGQAjQAzAfMBDQMuBbQHYgroDMwObg8rDkcMSAl/BdUBVP4i+4/5+/da91r3TPjt+I/5MPoi+3P7c/tR/Bv9lP0N/lT+9f4J/5D+5f2U/Rv9Kfxz+3P7sPsA/PP8kP4KAB8BGwLlAl4DDQNsAvMBwQFcAfcAcAEbAkQClAI2A4cDNgNEAnABpgB9/z/+bP3z/Hn8APyw+9j7KfwA/Cn8ovzz/PP8Q/3l/Q3+P/58/qT+uf4J/3P/sP/2/9j/4v9QAEcBRAKNBFYIRwxuD1IRKBQoFPMRzA4EC3EGbAKk/nP74Pmd+Kr3Wvf790z47fjg+dL6c/vY+6L8Q/18/jL/pv8UAFoAxP+5/rz9efzS+j75nfid+O34j/nS+qL8Df4d/xQARwHBAcEBrAHzAfMBGwKUAigEfwUgBnEGEwfCBi4FNgMfAQn/yvzS+o/57fhM+Ez4TPjt+I/5MPpz+xv9P/7h/uL/9wCsAZgBwQEbAtUBugCm/zL/aP68/Sv+pgCNBPcIKw42E64W8ReuFigUDxAEC9AF4wCi/I/5uPb89Pz0nvWe9T/2WvdM+J34Pvkw+nP7ovy8/XP/pgBHAcEB8wFcARQA5f0A/NL67fhM+Ez4PvmB+in8vP1p/5cAHwHBAZQChwONBNAFtAfBCQQLpQulC2IKYwcABHkAG/2P+Qn3nvX89Pz0W/T89J71P/Za94/5c/ts/R3/9wBEAuUClAIbAnABlwC6/xQA8wF/BcEJbg9rFTQZ/Rz9HHcaaxUrDnEGbgCB+j/2ufN38nfyGPO58/z0P/a49qr3TPiP+dL6Ufzh/kcBhwPeBH8F3gQNAxQAovyP+T/2ufN38rnz/PT793P7Wv+9Ai4Fwga0BwUIBQgFCFYISAnBCcEJSAm0B40EugB5/Ez4W/Q08fHv8e808XfyW/Q/9kz4j/ki+6L8Df7h/rD/jQAfAfcAUAB9/7n+VP7OAH8FpQsoFLobjiT4KPgojiT9HFIRIAYi+zTxlerM5szmUulr7XfyWvci+7z9Cf8oADMBRALXA9AFwga0BxMH3gRcAXn8CffV8WvtleqV6mvtNPFa9w3+jQTBCUcMKw6JDaULSAkTB9AFfwXQBXEGwgbQBTYDWv+B+p711fFP767u8e/V8Vv0Wvc++dL6c/tz+3P70voi+yn8KfzK/OX9yvy8/eH+WgAFCLEQdxr4KA8zGjigOg8z0SVrFXkADe4734LU/dEI18DhDe7Y+1YIUhFrFSgU8xHoDBMHXAFD/TD6Cff89Hfyru7Y61LpD+hS6crsGPPY+y4FzA6uFrobuhuuFokN5QL79/Hv2OvK7Lnz2Pt/BYkN8xHzEYkN0AWi/LnzyuxS6VLpUunK7JLw/PTt+Nj75f2Q/lT+lP1z+zD6Pvn794/5APyEAcwO/RwDLqw/PUc9RyY9fiuVEvv3ctvmx869VMB3z4nlMv+uFtElAy5+K8UgNhNsAvz02OuJ5UbkzOYP6Njryuxr7fHvGPNM+AAAwQmVEjQZ/Rx3GlIRIAaq91LpfuA738zmd/LXAygUxSAUJ0sj8RcFCJ71ieUv2gjXCNc731Lp/PQJ/40EwgZ/BZcA2PsJ97nz1fHV8dXx/PTt+A3+iQ1AHg8zw0miV5dSSUyJMKULUunmx8O4t7NUwC/aMPp3GgMuoDoaOPgoKBQ8ANjrO99y23LbfuDM5tjra+3V8Vv0c/suBcwONBmCH4If8RcEC4/5zOYv2oLU7NiV6gAArhb4KIkwAy7FIAQLW/R+4ILUgtS13NjrnfjlAhMHjQS8/fz0a+2V6srsGPPS+h8BlAKEATD6nvV38tL6lRJ+K0lMuWHFZpdSGji0B3LbSbteqLez8cwY864WDzOgOg8zxSDBCbj22OtS6djrNPHV8fHvUukD48DhzOZb9HEGNBnRJfgoQB5HDHfytdx3z/3Rtdxb9A8Q0SWJMAMuQB7BCRjzA+Ny2zvfD+j89Gn/fwXeBJz/qvfx79jrlequ7vz00vpG/+z/K/7797nzDe7x7/z0Kw5+Kz1HuWG5YZdSiTATB7XcVMDDuNrCfuCU/a4W0SV+Kwgi8ReJDS4F4wB5/D/2yuzA4QjX/dGC1APjWvexEEsj+CiOJDYTgfrA4f3R8cwI19jrxP8oFEsj+CgUJ4IfUhGUAvz0D+h+4PjdA+Ou7in8BQhiCrQHQ/008czmA+OJ5a7uWvcA/NL6P/bK7MzmzObV8UAet0TQa/N60GvDSUcML9pprWmtzr1G5EcMjiSJMPgogh8PEEcMVggTB1wBufM73/HMYMVryn7gcAFAHgMuiTBAHocDyuwI1/3RgtR+4E/v5f2mCJUSgh9+Kw8ziTDFIOz/+N1gxUm7a8qV6kcM0SV+KzQZc/s73/HMd89+4Fv0pgANA0z4Uul+4HLbD+gY8zQZw0nFZv9/3HCXUq4WfuA9tlKjVMDs2JQCgh8PM5U1lTV+K0AeKBTS+gPja8pUwM69gtTV8UcMFCd+K0sjKBTOAA/octuC1P3RCNdG5Pz0BQj9HPgofisDLsUgwQlb9C/a8cyC1EbkQ/0PEDQZKBQFCKr3ieX43bXcRuSV6lv04PmB+o/5ufPY69jryuxz+xo4rlzzetxwPUfMDgjXVMDOvcrslAKVErobNhNrFf0c+CjRJbobG/2C1FTAVMD90fz0UhHxF/EXKw6sATwAP/4Y8w/oL9p3zy/ad/LoDAgi+ChAHg8QwQnXA4f/j/kP6C/aL9qJ5Wz9iQ1rFQQL7fjM5nLbfuCJ5fHvyuyV6lLp2Ouq9yn83gS6/3EGrD+iV9xwxWaJMPcActuC1MDhcQbCBo0EfwU8ACgU+CgPM8UgYwc73/HM/dHM5p341wMJ/z/2GwJiClIRNhOEAUbkgtT90cDhP/4gBsEJBQgFCCgUjiQIIokNGPPs2AjXieU/9sT/APx38q7uufOP+dL6GPPM5nLbfuCJ5TTxqvee9VLpDe7x7zQZuWHQa+d1t0QECzD6Cffl/c7/nfiC1MzmrwPRJTJCoDr9HBv9ufMP6Bjzru473wjX+N2e9ZUSjiQoFJD+GPOu7lv0Mv938szmO9/43YH6axUIIksjuhv3CGQA2Ps08djrwOFy20bkNPHY+z/+/PQ08ZLwT+808a7uRuTA4YnlDe408ar3ufPBCZdSxWbccJdSNBljB0QC5QL1/vHvd88v2u34QB4DLvgoaxWHA0gJcQYTB/z0tdz90bXcT+9a/8EJuf6P+YQBwQnMDgQLnvVG5MDhzOYp/MEJpQulC4kNbg82E8wOQ/3K7MDhwOHK7D/2d/IN7srs8e/t+O34qvc/9srs8e938vz0nvVP79XxpQuVNTJCMkIDLkAexSCuFjQZsRBz+3fyNPHg+RMHVgivA3P7nfgw+mz9jQQyAI/5qvee9Y/5aP6Q/vv3P/Za9+34aP4d/yL7MPo/9qr3vP0LAdcD3gSHA5QChwPlAjIAvP2B+u34j/mB+iL7c/sJ9wn3nvV38gn3P/YY87nz1fHx79XxP/b89B8BNhOlC7EQQB7xFzQZSyPFIDQZdxooFMEJzA7MDn8FLgXl/ar3c/uP+Z710vow+rnzuPbt+J71nfjS+gn37fhM+Pz0nfgw+kz4P/7VAbn+vQIgBuUCjQRxBmwC8wFeA4f/Q/18/lH8P/Z38nfyNPHV8fHvDe5r7a7ud/Lx75LwCffBCZUSYgprFbobNhO6G0sjNBmuFncaUhFHDA8QYwcKAN4EkP6B+pz/7fi589j7gfpb9I/5P/Y08Uz44PkJ99L6Pvl38p714Pla97z9pgAi++H+XgNHATYD0AUfAVAARwFo/mz9lP0w+rj2Cfed+Ez4nvVb9Hfy1fG587nzGPNb9Pb/lRKJDcwO/RzxF64WCCLFIK4Wuhs0GUcMzA6JDUcBRALVAT75IvsA/LnzP/bY+/z0/PSB+vz0nvWB+rj2P/bt+Pz0d/KP+eD5TPi5/nn8IvvzAfMBeQBeA0QCuf5kAFoAKfzK/HP7P/b89Pz0GPM08XfyGPPV8RjzufNb9An3rwNrFegMUhHFIK4W8RdLI4If8Re6G64WpQtuD0cMbgCEAXP/Wvcw+tL6GPNa99j7W/QJ97D7nvVM+Cn8qvf79zD6Wvc/9sr8APww+jIApv8d/9cDNgPOAEQC8wE//pD+UfxM+J34Wve583fyGPM08fHvNPE08TTxd/L89Pz0sPtuD5USiQ39HEAerhZAHsUgNBmuFjQZKw7BCcEJNgPh/pz/c/ta93P7qvdb9CL7j/m580z4nfj89DD60voJ94/5gfqq94H6kP7Y+2z9cAGS/1wBjQREAnABLgUNA0YAlwAr/nP7sPuP+Vv0ufO58zTxd/K58zTx1fG58z/2+/dQBLEQpQvzETQZaxU0GYIf/RyuFjQZaxWlCwQLBQhHATL/bP249vz0P/bV8XfyP/YY89XxP/ae9fz0nfiq97j2nfjt+Ez4Kfyk/mz9GwLXA5QCfwXCBgAEAAQgBpQCRv9G/6L80vob/dL6Cfda9wn3W/T89D/2d/Jb9Lj2WvfY+/cIsRCJDa4WdxquFjQZQB66G64WNBk2E/cI9wgTB3kAQ/1D/Qn3W/Q/9hjz1fFb9LnzNPFb9J71W/Q/9gn3P/a49kz4qvfg+XP7sPu8/Qn/HgDBAZQC1wPeBC4FrwPBAY0ACf/l/dj74Pn79wn3P/Zb9J71Wvee9Qn3j/mB+rD70AXzEYkNKBT9HPEX8RdAHsUgNBl3GjQZ6AxICcEJDQOi/Gz9Pvl38vz0ufOS8HfyufOS8DTx/PRb9Lnzqvf79wn3nfjg+Y/50vrK/Bv9bP2S/zL/Rv9HAR8B4wAzAVoAaP7l/T/+UfzS+p34Wve49j/2nvW49gn3Wvc++Y/5gfpQAAQLlRJSETQZ/RzxF3cagh9AHjQZuhuuFqULYgqmCB8BG/3K/Lj2NPEY8zTxT+/V8RjzNPEY8z/2W/T89J34+/da9zD6c/s++VH8zf5s/Wj+jQCXAH3/RwGUAsT/kP4y/7z9UfzY+3P7j/md+D75PvlM+O34gfqw+9L62PuU/dj7Q/3zAfcI6AwrDigUrhZrFTQZuhs0Ga4WNBkoFIkNRwymCOUCWgA//vv3/PRb9DTxkvA08dXxkvA08bnzGPNb9D/2Wvft+NL6c/vY++X9Hf99/6YArAHBAc4ARwGYAQAAWv8d/+X9sPuB+tL6PvlM+O347fid+I/54PmB+gD8Q/18/nkAGwKUAlAEtAelC4kNsRAoFGsVaxWuFq4WaxUoFJUSKw5iChMHrwMyAJT90voJ97nz1fGS8E/vru6u7vHv8e+S8NXxufOe9ar34Plz+/P8uf48AJgBvQLXA1AEUARQBI0EjQSHA+UClAJwATwAuv8d/yv+vP28/cr8yvxD/bz9aP7N/lr/7P+6AFwB1QGUAocDUASNBH8FEwemCMEJBAulC6ULpQulCwQLYgrBCVYIIAbXA5gBWv9s/dj7MPpM+An3nvX89Fv0W/T89D/2P/YJ9/v3nfiP+dL6APwb/Q3+Mv/O/3kA4wAzAawB8wG9AuUCNgM2A70CbALzAR8BlwAeAOL/ff8y/+z/PABaAI0AugDjAM4AhAHBAcEBmAGEAZgBhAHBAZgBXAFwAUcBRwFHAXABhAGEAcEBrAGYAVwB9wCmAI0AgwD2/4f/Rv+Q/g3+5f28/ZT9lP28/Wz9bP2U/UP9lP0N/mj+uf4J/87/HgC6AB8BrAEbAkQCRALzAfMB8wHBAfMB1QGYAUcBHwHjAHkAPADs/5z/af8y/5L/sP9a/+H+pP7h/uH+Mv9z/4f/sP99/3P/af9z/5z/kv9z/1r/af8y/wn/zf65/s3+4f5G/3P/ff+6/woAPABaAI0ApgC6AM4AHwGEAZgBmAGYAcEB1QHBAcEB8wHVAfMB1QGEAUcBCwFQAOz/HgCDAHkAPABaADIA9v/i/5z/h/9a/0b/Hf/1/gn/zf65/qT+fP5o/mj+fP58/g3+Df4N/g3+P/7l/bz9Df6U/bz9Df4N/lT+kP7N/gn/Rv+S/8T/FADO/84ApgCm/40AugDOAB8BmAFsApQCvQK9ApQCvQKUAmwCbAIbAvMBbAJsAhsC8wHBAfMB1QHzARsCHwHOABQAaf9a/wn/4f65/pD+fP65/uH+zf65/rn+Df7l/ZT9bP28/bz9vP3l/Q3+VP7l/Q3+vP1D/ZT9vP3l/Wj+Mv/E/wAAWgAUACgApgDjAEcBmAGsAcEB1QGUAl4DhwMABNcDNgMNA+UClAINA+UCvQIbAgsBpgALAXABwQFwAfcAlwAeANj/Mv8J/6T+VP5U/g3+lP1s/UP9G/0b/Rv9G/0b/Rv9G/0b/Rv9Q/0N/mj+VP4N/sr8ovxs/bz9P/4J/5z/kv+H/7D/7P95AM4AHwFwAfMBGwIbAkQCbAKUAg0DNgM2AzYDhwPXA9cD1wPXA4cDhwNeAw0DbAJEAvMBwQGYAUcB4wCDABQAuv9p/x3/pP5U/j/+5f2U/UP9G/3z/Mr8efxR/FH8Ufx5/Mr8efx5/KL8efyi/Mr8G/1s/ZT9vP28/Q3+P/6Q/vX+Rv+6/wAARgDOAAsBRwGYAfMBbAKUAr0C5QINAzYDhwOHA68DrwOHA4cDXgM2A14DDQNsAkQCRALzAdUBhAFHAfcApgBkACgA4v+m/3P/c/9G/wn/4f58/j/+K/4N/g3+vP2U/Wz9bP28/bz9vP2U/Wz9bP1s/Wz9vP0N/j/+aP58/nz+kP7N/jL/Wv+c/+z/9v8oADwAeQDOAAsBXAFwAXABmAGEAZgBrAGYAZgBwQHzAfMBwQGsAXABhAFwAdUB1QGEAUcBHwH3AOMAugCmAHkARgAKAGQAZAAUAAAAxP+S/2n/Rv8y/zL/Hf/h/rn+kP4d/+H+aP6k/nz+fP7l/Wj+zf4N/rz9lP0N/sEBmAHVAY0E9wAb/dj7KfzN/goAzv+S/8T/DQO9AqYAHwGEAcEBKAS6/xv9CgBD/bz9ugDN/nkAvQJEAkcM6Awy/5Lw1fEd/7z94Plz+/MBPACB+rn+YweVEmMH0vq5/g3+Mv+S/5L/9wi0B2z92PsbAqYI9wAb/TL/pP6U/Z34Q/3XA5gBQ/0N/vMBvQLOAOH+uv/OAJT9gfo++TD6Q/0A/Bv9AACw/x8B8wFwAZQCrwMuBSgEBQjMDkcM9wimCNAFfwVeA7z9G/3h/rz9lP0b/eX9nP9kAJz/5f1D/Sn87fiq90z4TPjg+XP7MPow+nn8P/66/zIAlP1D/Zz/HgCS/3P/h/95AFr/vP0b/ZT9Cf+c/+z/eQCEAZQCvQJEAsEB1QH2/wn/2P8//mj+sP/l/Q3+9v98/hv9efww+jD68/y5/rz9G/3l/Wn/RAK9AsT/rAFxBi4FjQTeBEQCaxXRJf0cdxp3GjQZNBkoFI0ECfcJ9zTxlerM5vjdO99S6fHvNPE/9vP8rwNHDIkNwQlHDMwOpgjlAtj7W/SS8E/v2OtS6djr8e808Rjz/PS49tL6Kfw++Y/5Cfee9e34IvvK/Nj7G/2UAvcIBAsECzYTuhuJMCY9lTUPMwMu0SWOJK4WrAGS8JXq2OvM5lLpD+gP6PP8SAlHDMwODxCxELEQiQ0p/NjrieV+4LXcctu13MDhlepM+GwCiQ1rFa4WdxquFugMCgC49q7uA+Mv2i/awOEP6NjrT+/89Fv0nfjg+e34bP3z/FH8Hf9D/XP7ovwUAB3/FCcyQg8zoDqgOpU1GjiJMFYIT++S8A/ofuDA4S/agtSS8MwOsRDxF8Ug8ReuFrobBAt38tjrwOEI1+zY7Ngv2szmP/amAMEJ8Rf9HDQZdxooFHEGQ/389FLpO9/43bXcfuCV6jTxnvVU/g0Dc/9G//MBj/la91r3d/J38nfyNPEY8z/2lRK3RD1HJj0yQqw/rD+gOsUgW/RG5FLp+N0v2i/ad89y24H6rhZ3GjQZ/Ry6G4If/RymCHfyzOYD47Xc7Njs2C/afuC58zYDtAcrDjQZxSBAHmsVYgr3AAn3a+2J5XLb7NjA4VLp2Os08TD6bP28/dcDNgNR/Pv3P/a49j/2d/Lx70/vsPt+K0lMoDqJMBo4MkKsP/gorhbg+czm2OuV6jvf/dF3z8zm5f2lC5USzA4oFEsj0SXxF+UCqveq9zTxieVy2wjXtdzM5pLwP/Zz+7QHKBR3GvEXDxBuD0cMGwI++TTx2OvM5sDhfuAD44nlyuxP73fy+/dR/FwBzgB5/D/+Rv9z+6r3P/bBCQ8zrD9+K7obSyOgOho4uhtcAZ71nfgw+mvtfuAI1wjXleqw/7QHjQRaAIkNQB79HA8QLgUbApgBgfrx71LpRuRG5FLpa+3x73fyc/ulCzYT8xGJDcwONhPoDDYD0vp38q7uUunA4cDhRuTM5lLpleoY8zD68/xa/7z9h/+mABv9KfyQ/jQZJj0mPY4klRIIIjJCrD9rFTTx2OvOAI0EyuyC1PHMO9+B+i4FRALS+qL8sRDFILobBAtz+1T+YwevA7nzwOF+4Njrd/KS8NjrkvALASsO8xErDgQLbg9SEYkNKASe9a7ua+3Y65XqRuQ738DhUuk08RjzGPOq99j7vP1R/O34MPoyADQZJj23RIkwNBkIIjJCt0SCH5LwfuD89M4AT+/90WDFctuB+hMHAAQi+0cBrhaOJMUg6Awi+xv9fwUNAzTxtdxy21LpufMY89jrDe5o/m4P8ReVEsEJzA6uFmsVwgZb9K7ukvBP71LpwOHA4czmUunY62vtd/Kq9+34j/mP+Y/50vobAsUgMkI9R4kwNBnRJTJCrD/xF1Lp+N2583n8UunxzGDFO98b/fcIfwWk/gUIdxqOJMUgKw5a/5T94wAJ//HvO99y28DhyuyS8K7u1fFR/EcMaxU2E8wOKw6VElIRUARb9NjrDe7V8djrfuC13Ebka+1P78rsDe5b9HP78/zg+T755f26GzJCzk4aOHcaSyM9R0lMSyMP6ILUd/J/BTTxa8pJuy/asP8ECzL/W/SHA4IffisIIokNNgNxBsEJRAKu7jvftdzA4VLplepS6a7ugfoEC2sVaxXzEZUS8RdrFSAGW/TY62vtDe7M5rXcL9p+4FLpyuzK7E/v/PTY+3n8Ivv792j++CjOTklMAy4oFIkwl1I9R8wOCNdy287/3gR+4MO4zr3Y62IK0AVP77nzaxWJMIkw8RfQBcEJNhPMDoH6ieU738DhieXM5kbkRuTY65D+8xHxFzYTsRDxF7obKBRcATTxyuxP79jrfuAI1wjXO9/M5tjr2OtP77nzWveB+hv9zf40GTJCzk4aODQZ0SVJTM5Ogh+J5TvfmAH3CEbkw7jDuEbksP+d+Mzm2OtHDNElFCc0GcwOKBR3Gq4WVgj89JXqD+gP6FLpfuBy28DhufMuBVYIfwUECzQZCCJ3GsEJuf7l/ez/PvmV6jvfctt+4EbkA+N+4Ebkyuye9Uz40vrh/jQZMkLOTiY9SyMDLpdSl1LRJXfyru7oDFYI+N3DuFTAzOaS8Dvf/dFG5AQLdxrzEcEJDxAIIhQndxpiCrz9aP6Q/vz0D+i13LXczOYN7vHva+0Y84cDsRCVEmIKYwfMDlIRYgp5/HfyNPFr7czm+N1y237gA+PM5lLpT+/79woAQB4aOBo4fiv4KD1Hl1KVNVIREwdAHkAe1fF3zwjXNPHx72vK2sI73yn87fgD4zTxsRA0GUcMlAKVEsUg8xHBAc4Apgh/BRjzyuz89O34NPGV6rnzKfw++T/24PmUAuUC2Pts/TYDhAG49k/vnvVz+z/2De5P75717fhb9JD+dxpAHigUKBQUJyY9iTBrFfEX+Ch+KwQLa+3t+EgJbP213P3RleoJ94nlgtR+4D/+GwLY65XqIAY0GQQLqvflAjQZaxUJ/6r3wgYrDlr/kvA/9sEB8/xr7djrPvng+fHvleqe9YMA7fiu7vz0wQHOAPv3uPZuAPMB0vpz+w8Qgh9HDC4FQB6VNX4riQ2xEPgoFCfCBlv0jQTxFy4FA+NS6S4FGwID47XcuPZQBA3u+N3x7xMH9wDK7K7ufwUrDnP7NPHVAbEQBQhb9O346AylC571a+2U/bQHgfoN7rj25QJD/TTx1fHOAC4F+/d38hQAwQkyAFv0IvsTB3EGhwN/BdAFwQnMDgQLYwdHDDYTsRATB9AFDxAoFEgJnP+0Bw8QEwft+D75RwF9/z/2ufP793P7PvkY87nzc/vY+571nvUw+rz9c/uq9/P8XgOm/zD6efwfAVwBaP5D/RQA4wC8/XP75f2YAeH+j/nY+/cACf/t+D/2gfrY//MBfwUoBBMHKBSxEGwCEweuFpUSEwemCIkN6AxiCvcIcQbeBMEJ3gSP+dL6Df4++a7u2Ou583fy2OvK7NXxP/Y/9jTxnvUoAAsBP/4fAVYIpQu0B/MBUATBCfMB/PT794f/gfoY8/z0sPvz/OD5d/Ke9fMBqveu7lv0MPpiCsEJCf8rDn4rSyOVEvgolTWOJHcadxo0GcwO9wjS+hjzgfpP7zvffuBr7czmCNcP6DTxD+iu7k/vbP1xBu34NgM2E4kNUARICSgUKw42A1AEVggTB/z0nvXl/bnzlequ7vz0a+2u7g3ukvAJ92vt8e+d+DTxP/Z38rnzhAFWCP0cXgNAHjJC/Rz4KCY9iTCJMMUgQB7xF2sVDQOV6nP71fEI1y/aO99+4LXcctvM5vz0/PTY68EJaxW9Am4PdxrxF1IRRwxSEW4PjQSP+aYAWgCJ5TTxd/IP6MzmieV38pXqD+gJ93fy/PRb9Fv0Wv8Y83fyugCS8HfyP/a8/bobP/aVEg8zKBSJMEsjiTCgOncaFCfFILobRwwi+9cD+/dS6TvfieUP6C/afuAD42vtNPGJ5ZD+fwUr/o0EiQ2VEgQLRwyJDQ8QiQ1o/kgJwQn89HP70vqS8JLw2Ouu7vHvD+iu7hjz2Ouu7nfyd/KS8JLwP/YJ9/Hv0vpb9FIRiQ1R/AMuuhuOJAMuQB4yQsUggh8DLmsVuhsTBygEwQk08crsGPOJ5czmfuAD41LpieVG5PHvnvWS8Mr8G/1xBi4FKARSEaULRwxuD0cMiQ3CBsIGRAJkAO34W/QJ90/va+3K7K7ua+3Y62vtT+/x72vtNPFb9HfyufP89NL6P/YrDsIGEweOJCgU+CjFII4klTX9HNEl0SU0GXcaYgpiCkcMGPNM+Pz0lepr7cDhleoP6APjUulr7TTxkvBb9Gz9nP+k/tAFYgoECwQLiQ0PEEcMwQnBCWIKLgXz/Ln+c/v89JLw1fFP72vt2Otr7fHvlerV8a7u1fE/9k/v0vq58zD6WveHAygU7fjFIDQZgh/4KDYTrD/9HEAe+CjxF4IfYwcEC0cMIvuS8Fr3De5S6czmwOGS8PjdzObY667u1fEN7rz9vP1D/ZQCwQliCgUIiQ3MDokNBQgECwQLAAQJ/2QAKfye9bnzd/IY89jr2Ou58w/oyuyS8PHvd/KS8J34nfha94/5zv9z+70C8RfjADQZQB5rFQMuKBR+K/gorhbRJXcarhbzEdAFYgqHAzTx+/cY81Lp2OsD42vtD+hG5GvtufOu7rnzIvsb/UcBvP3BCcEJjQSJDQQLRwxWCFYIiQ2YATYDpgC8/fv3nvWe9TTxkvDK7DTxru5S6Xfy8e/x79Xx/PTt+Hfyc/sw+oH60vpxBq4WuPb4KHcaDxCVNYkNlTWCHygUiTAPECgUUhEFCNAFVP538nn8De6J5Vv0wOFP71LpD+g/9pXq/PSd+Hn8c/sbAigE0AVWCI0EDxD3CCAGiQ3CBhMH8wHVAez/Pvm49qr3W/RP7zTx1fFP72vtNPE08U/v1fH89D/2uPbt+DD6IvvY+3n8bP2YAcwOVgjMDkAerhaCH44kxSAUJ8UgQB6CH/EXsRCJDXEGbgCP+fz0d/KV6lLpUunM5szmD+iV6mvtru7V8e340voN/uUClAKmCHEGpgiJDVYIwQmmCC4FDQMoANL6j/mq93fyufPx7/HvT++u7pLwT+938j/2uPbt+HP74f6XAIcDjQQgBsEJwQlWCEcMUhFuD/MR8RfxFzQZQB53GvEXuhvxF4kN6AzoDLoA4Plz++34De7Y6xjzDe6V6vHv1fF38hjzP/bg+Y/54Pnl/ZT9bP0N/mz9pP4//qL8bP1o/rz9ovyU/aT+5f1s/ZD+Wv8J/zL/9v+DAFAAWgDOAKb/9v9a/5D+Mv+k/uX9kP4d/wn/Mv88AAsBlwDOAEcB8wHzAdUBbAKUAuUChwNeA9cDKATeBI0E3gQuBX8FLgUuBS4F3gTeBN4EKASvAzYDNgMNA4QBugCmAGQAPAAoAAAARv+5/pD+Df4//j/+5f3l/Q3+Df4N/uX95f2U/bz9lP1D/UP9G/1D/Wz98/zz/Bv9G/0b/Rv9G/1D/fP8G/2U/bz9lP28/Q3+K/5o/pD+pP71/jL/Rv/O/xQAWgC6AM4AMwFwAZgBwQHVARsCGwIbAvMBRAKUAmwCbAJEAkQCRAJEAkQCRAIbAkQCbAJEAkQCRALzAfMBrAGEAUcBHwELAc4AgwBkAOL/xP+m/wn/pP5U/mj+VP5o/uH+kP5o/lT+K/4//g3+K/5U/g3+Df4N/g3+Df7l/Q3+Df4N/j/+P/4//g3+P/5o/mj+pP6k/rn+Hf8J/zL/Rv+c/+L/4v8eAFAAeQC6AM4ACwELAR8BXAGEAZgBcAGYAcEBrAHVAdUBGwJEAvMBGwIbAvMB8wHBAcEB1QGYAZgBhAGEAUcBRwFHATMBMwFHAR8BugDOAJcAgwBQAOz/xP+c/5z/2P+S/3P/kv9G/x3/zf5o/pD+P/4N/uX9vP28/Wz9G/1s/bz9vP3l/Q3+5f0//lT+Wv+k/pD+9v9a/wn/uf4d/2QApgAyAHkA9wBcAc4A9wDVAQsBRwFcAZgBMwEzAR8BMwFwAUcBRwHVAawB8wFsAvMBwQGsAdUBhAFwAXABrAEfAaYAgwCmAKYACgAAAJD+Hf9p/2QAhAHK/IH6efwb/Q3+zf6U/Sv+zf4N/rn+4f6k/vX+9f4d/84AAAAN/in8Df71/g3+VP6k/m4A2P/1/sT/HgAUAAAARgCmAI0ApgDjALoAugC6AEQC8wH3AKwBhAEzAZgBcAEfAb0C1QEfAZgBMwFcAYQBugD3AEcB4wDOAI0AlwAbApQCRwGw/4MA4wBo/rz9uf71/vP8Q/2m/wn/vP3N/n3/pP4b/Q3+CgCDAMr8Hf+9Amj+APyEAdj/sPvs/8EBVP48AEcBpv/s/zL/KADjAFAAhAEzAYcD1wPh/gsBNgMoAGj+vQIoBLn+FAAuBaYAZAC9AsEBnP+XAPcAUACXADwA3gRo/nP7GwKXAFH8CgDeBCn8MPqEAWn/j/l9/0QCgfrK/PcADf7z/EQCHgA++bD/HwFG/9j/Q/0d/9UByvwp/DYDhAHY+0b/CwG6/ygAfP7Y/w0DrAHY+x8BKATl/c4AlALVAUQC8wEbAl4DXAH2//MB8wFHAYQBbAIzAWz94wBEAuH+1wMbAjD6Hf/XA6b/UACmADL/RAK6/7z9PACXALn+Rv98/gD85f1aAD/+Wv82A1T+yvwbAvcAlP3jADMBIvts/YcDPAD1/pQCvQLs/wn/RAJEAuH+5f2Q/tj/ugC6/4f/h/9o/pz/CwGNAI0Azf65/gAAfP5U/roAFACw/6YApP6XAEQCuv99//cAaP4b/cEBCwGH/5gBpP5s/XP/9f4J/3kAc/9QABsCh/8A/DL/DQNwAa8D3gS6AHP/lwBa/1H8yvym/7D/RgBHAY0A4v+S/ygAugBHAboAZABQADL/CwEbAvX+1QGNBMT/HwHlAg3+7P+YAQ3+4v8zAbn+XAHzAdj/rAHVAYf/MgAeAA3+aP58/hv95f2i/Hn8lP15/Mr8ovwb/Wz9c/vY+/P8APwA/D/+G/0N/h4ADf72/5gBuf4y/1wBCwGNAOUCAATXAwAEDQPXA40EfwUuBXEGSAljBwQLBAtiCugMpQulC8EJBQhjB40EhAEy/xv9j/nt+An3W/T89LnzGPNb9Pz0Wved+DD6yvwr/lr/ugD2/6YA4wCw/yv+G/3K/I/5Wved+Pz0/PS49j/2P/b79z750vpz+3n8RgBz/0cB3gTzAeUC0AUFCKYIRwxSEQ8QKBRrFfEXrhYoFK4WUhGJDSsOEwdHAY0AIvsJ9571d/KS8DTxT+/x73fyGPM/9jD6c/sy/6wBXgPeBLQHYwcuBcIGjQQfASgA5f2P+bj2P/YY83fyNPHV8RjzkvA/9j/2nvXt+D75c/si+9j7VP5s/Sn8kv/O/7n+6AxjB6YI/RzzEa4WCCLxF7ob/RyuFq4WUhFiCmIKHwHg+cr8GPM08Xfya+3x7/Hv8e9b9Pz0Cffz/A3+Mv9sAigEmAFsAigEPAC9AuL/Wv8UANL6UfyB+gn3qvcJ9/z0nvU/9p71uPZa9wn3Pvmd+I/57fjS+jD6WvdD/Uz4KfzY+y4FzA7l/UAeuhuxEPgoQB7FIIIfuhv9HCgUzA6lC0gJ7fjY+6r3ru7V8djrT+9r7Wvtd/IY81v07fi8/Rv9Mv/BAZgBc//3AEcBCgC6ANL6hwNR/Ez41wNa94/5vP3t+I/5MPo++SL7Pvla98r8/PQ/9oH6NPE/9j/2NPH89Fv0ufOq93P7iQ1kAFIRFCdSEfgoFCcIIvgo/RzFIDQZzA7oDLQHTPjt+D/22Ot38lLpru408djrCfee9T/2fP7N/s4ANgO9Al4DDQMJ//MB9v8i+1T+2Psw+tL6MPrS+j75Ivui/HP7APw//mz9yvwN/sr8Ivud+IH6uPYY80z4kvC587nzNPG49gn3BQh5/IkNxSDoDNElgh/FIEsj8RcIIigUbg8rDnEGovzt+I/5T+8Y8w3ukvB38tjrTPj89D/2vP2U/aYAHwENA14D1QGmAPMBPACi/DL/yvwi+9j7G/1z+yL7lP2i/Mr8efzl/Wz92Pts/Sn8MPrt+OD5Cfd38kz41fEY8/z0kvBM+LnzYgps/X8F+CjBCUsj0SX9HPgoKBRLI64WBAsPENAFc/tb9NL6yuyu7k/va+2581Lpnfj79xjzRwHh/sEBDQONBNAFRAJwAZQCHwHS+lr/bP3793n8MPp5/IH6c/tz/yn8vP1G/zL/5f2Q/jL/Kfyw++D5MPoJ97nzTPjV8dXxufMN7gn38e8gBo0EsPt+K8wOuht+K7obfitrFcUgQB4FCMwOYwdz+3fyTPiu7pXqru7Y63fyler89NL6W/Rz/+UClAIoBMIGEwevA+UCvQLzASn8Q/0r/kz48/x5/JT9bP3l/YcDQ/32/1AEbP0y/7D/vP1z+zD6gfq49vz0P/Z38tXxd/Ku7rnzyuy587nzgfpSET/2QB4UJ2IKiTAIIsUgxSCuFggipQsFCBMHuf5P79Xx/PSJ5a7uleo08a7uT+9o/kz4ovyNBNAFXAETB9AF5QKvA87/9v8//jD60vqi/FH84PlkAIcDG/3CBtAFlALQBdAFDQOmADMBbP3S+kz4MPq583fynvXx73fy8e808dXx8e8Y80z4GPOlCysOyvyJMDQZ8ReJMLobFCdrFa4W8RcNA8EBVP6B+szmufM08czmT+9r7Y/5De6d+AAEMPpEAtAFwgZcAY0EjQSU/Qn/c/s++Vr3CfcJ9z/2kP71/hv9pgimCC4FKw5iClYIwQnlAocDpP7t+OD5/PR38vHvNPHY60/v8e/Y67nzNPEY83fy7fjg+ar3/RyvA24PJj3zEfgoAy6CH44ksRC6G4kNCgBM+Nj/1fF+4Hn82OtS6Z71nvUi+/z08wE2A68DUAC0B/cIj/k2A8T/Wveq96r3GPO58/z01fHY+9L6lP0ECyAGpQvzEYkNzA6mCGIKhwMp/HP7CfcY82vtkvDx79jrru4N7vHv8e/x77nzufP89HfyKfye9Q3+SyPz/HcarD/zEQMuAy5LI8UglRKuFgQL1QFP787//PS13Mr8NPHK7An3sPuQ/g3+jQR/BegMhAHjAAQLMPpa99L6/PSS8DTxNPE08T/2d/Ii+5gBCgCNBA8QiQ1iCigUKw5jB2IK5f25/kz4kvC585LwyuwN7j/22OvV8bnzru649mvtGPNb9A3ukvBb9D/2W/RLI8EJ6AzDSUAe+CgPM/gojiQoFCgUYwdjB1LpGPMp/LXcGPM/9qr3nfhp//cIUAQEC0QCbg/BCUz4fwUb/fz0T+/x7zTxUukN7q7uqveP+Qn3cQamCLQHBAvzEVIRYgrBCWIK3gTg+e34Pvl38g3u1fGe9Wvtd/KB+tXxd/Jb9LnzGPPK7JLwGPPK7NjrnvUi+9Xxfiu6G/cIl1KJMNElGjh+K8UgsRCJDY/5bALM5sDhc/tS6djrMPq0B4f/AARSEcEJ8xHi/2QApgi589jr8e+e9czmieVP7zTxCfeS8Mr8EwcuBVwBcQZuDygEjQR/BQQLgwDY+9cDVP779z/2c/ud+Hfy+/ed+Pv31fGq9wn3NPEY85LwufNr7a7u2OtP7/Hv1fHg+TYDJj13GrobuWGVNfgoDzMUJzQZKATg+Wvtaf9y2/jdyvz89Kr3zgA2E8wO8xEPEMEJ8xE++fHvnvUN7rXcfuDY60bka+3x70z40AWHAz/+VgixEFAAAABQBPX+zf7Y+3n8WgAN/j/2bP0bAtL6bP2c/wn/4Plz+0z4nvWq9/HvkvDV8fHvDe6u7jTxa+0Y83fyj/kp/P0cPUeuFho4rlwPMwMuCCK6G/cIbP0D4wPjsPty20bk0vrBCSAGcQbxFygUNBnVAeD5XgPx78DhwOFS6YnlUukN7rnz3gTBAR3/IAZICSgEc/uU/VH88/zt+D/2mAEEC14DQ/2JDQQL5f3z/LD7Ivue9WvtDe4/9p71a+389Mr8ufOe9T/2W/T89Njr2OvV8Qn3GPOuFpdSgh9+K7lhoDp+K64WDxBG/6r3ctvxzHn8De5G5PP8NBk0GYkNdxpHDK4WhwM731Lpd/LA4QjX2Ov89NL6lwAi+0cMNhPzAQn3HgDOAE/vd/Ku7u34wgbl/XEGaxW6G0cMpghxBuD57fiV6szmW/Rb9E/vyvxQBDL/8wGB+gn3+/fK7EbkwOHM5sDhleq589j7l1JJTPEXoleiV/go8xEgBpXq2Os08c69RuRSEfb/fP53Gvgo/Rx3Gtj7WvcFCMDha8py23fy2OvK7Gz99wh3GsEJc/u0B3EGNPHA4czm8e+q97nzP/YEC7obaxVrFUAeNhNWCIH6UunY62vtRuSV6o/5zgBICcEJvQIuBUYAGPMP6Mzm+N343QPjA+PY67z9RwyuXKJXCCKXUj1HSyMuBTTxwOEP6DTx5scN7vEXiQ2xEK4W/Rx3GvEXa+1G5Nj7A+OC1H7gd/JD/UcMvQJEAkAeKw538nfy/PRr7VLpzOYD4zYDBAvl/aULQB6OJA8QcQYbAj75nvV+4MDhufND/bj2sPulC4kNSAk++fHv8e9P78DhgtRy28rs8e9r7Uz4gh/ccD1HxSA9Rw8zCCKB+gjX+N2Q/jTx8czl/cUg/Rz9HCgUaxUIIg8QL9p+4DTxwOGJ5cDhyuwoFK4W8/wuBWsVBQj798zmfuBr7U/vlerx7/cIrhauFggiKBToDA8Q/PTK7JXqUumV6nfynvVU/swOVggNA5QCufNr7ZXqL9py20bkUul+4A3uhwNuAJdSrlzFIDJCiTD4KEcMRuSC1JLwKAS13Gz9lRLxFw8zNBmJDQ8QiQ1r7cDhfuDs2HfyP/au7hMHKBSJDVIRVgjV8Y/51fF+4Mzma+249ocDRwwoFEAeNBmJDTYDMPo08VLpRuTM5mvt4PlaADYDVghjBzIAT+8P6Ebk+N1+4H7gO99S6dj/NgMmPcVmlTWgOg8zuhvxF5Xq/dFG5O348e9U/pUSaxUPM/goRwzQBUcB1fED47Xc/dFr7b0C+/dWCJUSKBTxF8IGkvCu7nfyRuR+4NjrnvVSEbobaxW6G5USDxDCBmvtA+MD41Lpa+0N7gn31QHoDDYDGPMN7lLpUunA4S/a+N3M5jD68wG3RLlhoDoyQvgoSyOuFgPj/dF+4LnzT+/zAegM8RegOtElbg9jB4/5W/TM5gjX/dHY64H65f1iCsEJKBS6GysOMPoN7lLpzOZS6crsnvW0B64WNBmuFjYT6Ay6ALnzzOY73w/oD+jY67nzP/ZcAQn32OvK7MzmieUD4w3unvV/BTJCw0mVNaA60SUUJ0Ae1fGC1EbkkvC49sEJlALoDH4rxSDMDo0EP/YY8xjzfuAI11LpP/a0B/MRBAtjB8wOpQvY+1v02Os08Q3+ovxcAcIGpQuxEBMHIvue9Vv0nvVP79jryuyS8An3kvCJ5VLpleqV6lLpleq58ygEoDrOTho4DzPFIEAeuhu583LbieVb9Ln+pQtICcwOFCeCHwQLxP808Q3uru5+4C/a2OtM+EQCBAsuBXEGzA73CIcDj/m58zD6PvlM+D75Df5QBNAFDQPz/Kr3nfj797nzNPGV6onlD+iJ5UbkUunY6wn3hwM0GT1HSUygOgMuNBlrFQQLleo735XqP/Z/BQ8QRwxrFQgiaxXBCQ3+ru7K7FLpO9/A4a7uP/YbAvcI3gQFCMwOBAtHAQn32OtP7wn3/PS587j25f0uBSgEIvv89J71CfcY81LpwOED41Lpru4N7rnzRAIUJ85Ow0mVNUsjNBnxF7QHD+i13FLpTPgECw8QiQ00GcUg/Rw2E3z+ru7Y68zmRuSJ5czmd/LXA0cMiQ2VEokNVgjBAfHvD+hS6ZXqNPG58/z09f5/BVAE9wAi+wn3GPOV6n7gctu13Ebk2Oue9aULDzM9RzJCDzNAHvEXKBR9/9jrRuRS6eX96AzMDg8Qrha6G/0cbg/g+a7ulerK7GvtleqV6gn3cQZHDJUSiQ3CBi4FIvv89DTxD+iV6jTxd/Kq99L6j/nK/Mr8qve589jrA+MD40bkieWV6rnzlRKVNSY9GjgUJzQZNBnzEbz9yuwD49jr8wGJDYkNKw5SETQZ/RwPEA3+1fEN7jTx/PTV8TTxj/lxBq4WNBkPEC4FKfwi+0z4De7M5szmleq49kP9efwi+zD6j/n795LwieU73zvfwOHM5gn38RcPMxo4iTDFIHca8RelC9L62OtG5K7ukP5xBsEJYgqlCygUaxWJDTYDnvWu7nfy/PRa99L65f1ICbEQUhGJDeUCovzg+fz0NPEN7tjrNPH797D75f0N/nP77fj89E/va+3K7GvtDe4/9kcMxSD4KEsjuhs0GfEXUhHBCT/+ufO581r3c/uS/1r/fP4zAY0E0AUuBbD/gfqd+J34c/t8/o0AGwK9AkQCGwIfAWz9j/m49lv0nvVa90z4j/ng+TD6Kfzz/Hn84PkJ9z/2nvWe9T/24Pm9AisOaxUoFFIRsRCJDcwODxClC7QHUAQzAawB5QI2A+UCh/+w+3n8bP2U/ZD+Q/1z+3P7bP0LAZQCHwHN/rD7MPoi+3P7gfow+qr3P/ZM+I/54PnS+kz4P/Y/9j/2P/a49j/2TPg//i4FwQmlC+gM6AzMDigUaxU2E7EQpQtICcEJwQnBCcIGHwG8/cr8efy8/eX9KfyB+tL6yvyNAGwCMwFG/xv9sPvY+wD8gfqq9z/2nvWe9Z71P/b89Lnz1fE08XfyW/Rb9Pz0nfiXAI0EIAbBCQQLpQsPEJUS8xEPEEcMwQmmCKYISAkTB5QCpv8N/pT9zf5GACgA9f58/sT/RALXAygEXgOsAQoA9f4//sr8gfrt+Ez4Wve49j/2/PRb9LnzufP89D/2P/b79w3+0AVWCBMHVghICQQLbg9SEcwOYgogBq8DhwPeBC4F5QLl/dL6gfqw+w3+2P8d/2z9bP2m//MBNgMNA6wB9v8d/z/+G/3Y++D5nfiq9wn3Cfda97j2P/a49qr3j/ki+3P7APzs/3EGSAlICWIKBAvoDA8QUhHMDqULBQh/BSgEvQKEAXP/Ufwi+3P7Ivuw+8r88/xs/ZD+Wv99/1r/Mv/1/mz9Ufx5/HP7MPrg+Y/5nfhM+Pv3qveq90z47fiB+lH8lP2k/nP/PACEAS4F9wjBCcEJYgpiCgQL6AzoDAQLwQkFCHEGfwUuBY0E1wNcAXz+Q/2U/Wj+pv+mAB3/bP0b/bz9Df68/VH8MPpM+Kr3qveq91r3uPae9fz0nvW49lr3TPiP+dL6sPvz/JD+WgCHA9AFwga0B1YISAliCgQLBAtiCvcIVgi0B3EGIAZ/BY0EXgOUAvMBRwHjAFwBGwLBAfcAWgCc/zL/4f4r/sr8Ivsw+o/5nfj796r3uPY/9rj2Cfeq9+34j/mB+rD7yvwN/qYAvQKvA40ELgVxBrQHVggFCBMHIAbQBS4FjQReA2wCwQELAY0AMgAKAAoAWgAzAR8BUADE/1r/Hf/N/j/+Q/0p/HP7c/si+9L60vrS+oH6gfqB+tL6IvvY+6L8bP3l/eH+PADzAYcDjQQuBdAFcQYTB7QHtAcTB3EG0AV/BS4FUASvA+UCGwKEAQsBpgD3AEcB9wBuALD/Cf/N/mj+vP3z/LD70vqB+jD6Pvnt+J34TPhM+O34j/kw+iL7Kfwb/Q3+kP5a/6YARAKHAygE3gR/BXEGYwe0B7QHYwfCBnEG0AUuBY0EAAQNA0QCrAHzAfMBhAELATwAh/8y/+H+K/5D/Xn82Psi+9L6gfrg+T757fjt+O34Pvng+YH60vpz+3n8Q/0r/pD+9f6XAPMB5QLXA1AEjQTeBC4FfwV/BX8FLgWNBCgEUARQBCgEAATXAzYD5QJsAhsCwQEfAXkAzv8J/7n+aP7l/Wz9yvwA/HP7IvvS+tL60vrS+iL7c/vY+3n8G/28/Q3+aP71/of/eQBwAfMBbALlAjYD1wNQBI0EjQRQBCgEKAQABAAErwMNA5QC8wFwAQsBjQDs/zL/fP7l/Wz9G/2i/Cn8c/si+9L60vrS+tL6Ivtz+9j7Ufzz/Gz95f0//qT+Mv/O/6YAXAHzAWwCDQOHA9cDjQTeBC4FfwV/BS4FfwUuBd4EjQQABDYDlALBAfcAeQDO/1r/zf4//rz9Q/3K/Hn8Kfyw+3P7IvuB+tL60voi+3P7APx5/Bv9bP3l/Wj+Hf/2/6YAcAEbAg0D1wONBI0E3gQuBS4FfwV/BS4F3gRQBK8DNgO9AvMBcAG6APb/Hf98/g3+lP1D/fP8efwA/Nj7sPs=\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "print('Utterance with lowest CER: {}\\n'.format(low_cer))\n",
        "Audio(low_wav, rate=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvsavm_F_BjF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "3ebcff07-d697-4e05-f8fe-03db91030d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utterance with highest CER: 1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "print('Utterance with highest CER: {}\\n'.format(high_cer))\n",
        "Audio(high_wav, rate=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DpQsZADTfKA"
      },
      "source": [
        "**→ What are the lowest and highest CERs? Why do you think CTC got these CERs for these utterances?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VFJdCCr1yFk"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:**\n",
        "Clear voice vs muffled voice. The one with the higher CER has very unclear voice thereby CTC is not sure what character to assign at each timestep thereby CER whereas the in the first exampe which has a clear audio, CTC is able to assign with high probability at each time frame; thereby reducing CER.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TtjP0eSZ_tI"
      },
      "source": [
        "## **Task 3.2: Run inference using your model [5 points]**\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "**→ Similar to `get_low_high_cer_wav`, we'll run inference on a single audio file and see what the model transcribes.** Fill in `run_inference` to have your system decode test utterances from a `.WAV` file. We will later run this function in Parts 5 and 7 to qualitatively evaluate systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBJwFvv2aHsK"
      },
      "outputs": [],
      "source": [
        "def run_inference(\n",
        "    system, wav, device=None, sr=8000, n_mels=128, n_fft=256, win_length=256,\n",
        "    hop_length=128, wav_max_length=512, labels=None, label_lengths=None):\n",
        "  \"\"\"Run your system on a .WAV file and returns a string utterance.\n",
        "\n",
        "  Args:\n",
        "    system: a pl.LightningModule for your chosen model.\n",
        "    wav: a .WAV file of an utterance\n",
        "    device: GPU -> torch.device('cuda')\n",
        "\n",
        "  Returns:\n",
        "    A string for the utterance transcribed by your model.\n",
        "  \"\"\"\n",
        "  input_feature = None\n",
        "\n",
        "  mels = librosa.feature.melspectrogram(y=wav, sr=sr,\n",
        "                                        n_mels=n_mels,\n",
        "                                        n_fft=n_fft,\n",
        "                                        win_length=win_length,\n",
        "                                        hop_length=hop_length)\n",
        "\n",
        "  mels = librosa.power_to_db(mels, ref=np.max).T\n",
        "  mels = librosa.util.normalize(mels, axis=1)\n",
        "  input_feature, input_length = pad_wav(mels, wav_max_length)\n",
        "  input_feature = torch.tensor([input_feature], dtype=torch.float)\n",
        "\n",
        "  input_lengths = torch.LongTensor([input_length])\n",
        "  # Whether or not to use GPU.\n",
        "  if device is not None:\n",
        "    input_feature = input_feature.to(device)\n",
        "    input_lengths = input_lengths.to(device)\n",
        "    if labels is not None:  # to test teacher-forcing\n",
        "      labels = labels.to(device)\n",
        "      labels_lengths = label_lengths.to(device)\n",
        "\n",
        "  utterance = None\n",
        "  ############################# START OF YOUR CODE #############################\n",
        "  # TODO(3.2)\n",
        "  # Run your system on the utterance input feature to get log probabilities\n",
        "  # and decode the log probabilities into indices. Then turn those indices into\n",
        "  # characters.\n",
        "  log_probs, _ = system.model(input_feature, input_lengths)\n",
        "  hypotheses, hypothesis_lengths, references, reference_lengths = system.model.decode(\n",
        "        log_probs, input_lengths, labels, label_lengths,\n",
        "        system.train_dataset.sos_index, system.train_dataset.eos_index,\n",
        "        system.train_dataset.pad_index, system.train_dataset.eps_index)\n",
        "\n",
        "  # Get the first hypothesis (since we only have one audio sample)\n",
        "  hypothesis = hypotheses[0]\n",
        "\n",
        "  # Convert indices to characters\n",
        "  chars = system.train_dataset.indices_to_chars(hypothesis)\n",
        "\n",
        "  # Join the characters to form the utterance\n",
        "  utterance = ''.join(chars)\n",
        "\n",
        "  ############################## END OF YOUR CODE ##############################\n",
        "  return utterance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg_FAAx04iP9"
      },
      "source": [
        "# Part 4: Leveraging Auxiliary Tasks for Multi-Task Learing\n",
        "\n",
        "When designing a speech system, we might care about more than just the transcription. As a bank, we might want to know the intent of the caller, for example.\n",
        "\n",
        "Our dataset provides labels for these auxiliary tasks, including dialog action, the intent of the caller, and the sentiment of the caller. In the spirit of an end-to-end system, we will expand the CTC model to make predictions for these auxiliary tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO-ng6Sgbv6d"
      },
      "source": [
        "## **Task 4.1 Working with auxiliary task data [5 Points]**\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "**→ Fill in `__getitem__`. Add one or more auxiliary tasks to your training.** We include `get_auxiliary_labels` for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co1TnD6lf-Kn"
      },
      "outputs": [],
      "source": [
        "class HarperValleyBankMTL(HarperValleyBank):\n",
        "  \"\"\"Like the HarperValleyBank dataset but returns labels for task type,\n",
        "  dialog actions, and sentiment: our three auxiliary tasks.\n",
        "\n",
        "  See `HarperValleyBank` class for description.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self, root, split='train', n_mels=128, n_fft=128, win_length=256,\n",
        "    hop_length=128, wav_max_length=200, transcript_max_length=200,\n",
        "    append_eos_token=False):\n",
        "    super().__init__(\n",
        "      root, split=split, n_mels=n_mels, n_fft=n_fft,\n",
        "      win_length=win_length, hop_length=hop_length,\n",
        "      wav_max_length=wav_max_length,\n",
        "      transcript_max_length=transcript_max_length,\n",
        "      append_eos_token=append_eos_token)\n",
        "    self.auxiliary_labels = self.get_auxiliary_labels()\n",
        "\n",
        "  def get_auxiliary_labels(self):\n",
        "    \"\"\"Returns auxiliary task labels.\n",
        "\n",
        "    This function will take the raw auxiliary tasks and convert them\n",
        "    integers labels (for neural networks).\n",
        "\n",
        "    These include: `task_type`, `dialogue_acts`, and `sentiment`.\n",
        "    \"\"\"\n",
        "    # task_types: each element is a string representing a conversation-level\n",
        "    #             label. So all utterances in the same conversation share\n",
        "    #             the same label.\n",
        "    task_types = self.label_data['task_types']\n",
        "\n",
        "    # dialog_acts: each element is a comma-separated string of dialog actions\n",
        "    #              that describe the current utterance\n",
        "    dialog_acts = self.label_data['dialog_acts']\n",
        "    dialog_acts = [acts.split(',') for acts in dialog_acts]\n",
        "\n",
        "\n",
        "    # sentiments: each element is a 3 dimensional vector that sums to 1\n",
        "    #             representing the probabilities for\n",
        "    #             \"negative\", \"neutral\", and \"positive\"\n",
        "    sentiment_labels = self.label_data['sentiments']\n",
        "\n",
        "    # Get label vocabularies.\n",
        "    task_type_vocab = sorted(set(task_types))\n",
        "    dialog_acts_vocab = sorted(set([item for sublist in dialog_acts\n",
        "                                    for item in sublist]))\n",
        "\n",
        "    task_type_labels = [task_type_vocab.index(t) for t in task_types]\n",
        "\n",
        "    # dialog_acts_labels: list of 1-hot vectors\n",
        "    dialog_acts_labels = []\n",
        "    for acts in dialog_acts:\n",
        "      onehot = [0 for _ in range(len(dialog_acts_vocab))]\n",
        "      for act in acts:\n",
        "        onehot[dialog_acts_vocab.index(act)] = 1\n",
        "      dialog_acts_labels.append(onehot)\n",
        "\n",
        "    # Store number of classes for each auxiliary task.\n",
        "    # Note:\n",
        "    #   - task_type is a N-way classification problem.\n",
        "    #   - dialog_acts is a set of binary classification problems.\n",
        "    #       (more than one dialog action may be \"on\" for an utterance)\n",
        "    #   - sentiment is a regression problem (match given probabilities).\n",
        "    self.task_type_num_class = len(task_type_vocab)\n",
        "    self.dialog_acts_num_class = len(dialog_acts_vocab)\n",
        "    self.sentiment_num_class = 3\n",
        "\n",
        "    return task_type_labels, dialog_acts_labels, sentiment_labels\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"Serves multi-task data for a single utterance.\"\"\"\n",
        "    if not hasattr(self, 'waveform_data'):\n",
        "      self.load_waveforms()\n",
        "\n",
        "    index = int(self.indices[index])\n",
        "\n",
        "    primary_task_data = self.get_primary_task_data(index)\n",
        "    auxiliary_task_data = None\n",
        "\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.1)\n",
        "    # Get auxiliary task label(s) for this index using\n",
        "    # `self.auxiliary_task_labels`. Populate the object `auxiliary_task_data`\n",
        "    # as a tuple of auxiliary task labels. Make sure to cast appropriate\n",
        "    # torch tensor types for the different labels.\n",
        "\n",
        "    # Unpack the auxiliary labels\n",
        "    task_type_labels, dialog_acts_labels, sentiment_labels = self.auxiliary_labels\n",
        "\n",
        "    # Get the specific labels for this index\n",
        "    task_type = task_type_labels[index]\n",
        "    dialog_acts = dialog_acts_labels[index]\n",
        "    sentiment = sentiment_labels[index]\n",
        "\n",
        "    # Convert to appropriate tensor types:\n",
        "    # - task_type: integer classification, use LongTensor\n",
        "    # - dialog_acts: multi-label binary classification, use FloatTensor\n",
        "    # - sentiment: 3-dimensional probability vector, use FloatTensor\n",
        "    task_type_tensor = torch.tensor(task_type, dtype=torch.long)\n",
        "    dialog_acts_tensor = torch.tensor(dialog_acts, dtype=torch.float)\n",
        "    sentiment_tensor = torch.tensor(sentiment, dtype=torch.float)\n",
        "\n",
        "    # Combine into a tuple\n",
        "    auxiliary_task_data = (task_type_tensor, dialog_acts_tensor, sentiment_tensor)\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    if not isinstance(auxiliary_task_data, tuple):\n",
        "      auxiliary_task_data = (auxiliary_task_data,)\n",
        "\n",
        "    return primary_task_data + auxiliary_task_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MND0Dny4FBrN"
      },
      "source": [
        "## **Task 4.2: Implement auxiliary task heads [5 points]**\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "**→ Fill out the `Classifier` classes.** You will use these in `LightningCTCMTL`.\n",
        "\n",
        "Each classifier should be a simple one-layer model.\n",
        "\n",
        "Use a linear layer to map input features to the number of output classes.\n",
        "\n",
        "Choose the activation function carefully depending on the task:\n",
        "\n",
        "- The `TaskTypeClassifier` involves selecting one class from N classes (e.g., intent prediction).\n",
        "\n",
        "- The `DialogActsClassifier` involves making independent binary predictions for multiple labels.\n",
        "\n",
        "- The `SentimentClassifier` requires predicting a probability distribution of emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-cjrZkscJUp"
      },
      "outputs": [],
      "source": [
        "class TaskTypeClassifier(nn.Module):\n",
        "  def __init__(self, input_dim, n_classes):\n",
        "    super().__init__()\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "\n",
        "    self.linear = nn.Linear(input_dim, n_classes)\n",
        "\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    log_probs = None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    # Hint: This is an N-way classification problem.\n",
        "    logits = self.linear(inputs)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    return log_probs\n",
        "\n",
        "  def get_loss(self, probs, targets):\n",
        "    loss = None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    loss = F.nll_loss(probs, targets)\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    return loss\n",
        "\n",
        "\n",
        "class DialogActsClassifier(nn.Module):\n",
        "  def __init__(self, input_dim, n_classes):\n",
        "    super().__init__()\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    self.linear = nn.Linear(input_dim, n_classes)\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    probs = None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    # Hint: One person can have multiple dialog actions.\n",
        "    logits = self.linear(inputs)\n",
        "    probs = torch.sigmoid(logits)  # Multi-label classification\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    return probs\n",
        "\n",
        "  def get_loss(self, probs, targets):\n",
        "    loss = None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    # Hint:\n",
        "    # - probs shape: (batch_size, num_dialog_acts)\n",
        "    # - targets shape: (batch_size, num_dialog_acts)\n",
        "    loss = F.binary_cross_entropy(probs, targets)\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    return loss\n",
        "\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "  def __init__(self, input_dim, n_classes):\n",
        "    super().__init__()\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    self.linear = nn.Linear(input_dim, n_classes)\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    probs = None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    # Hint:\n",
        "    # - Sentiment is measured as a log probability distribution among multiple\n",
        "    #   possible sentiments.\n",
        "    logits = self.linear(inputs)\n",
        "    probs = F.log_softmax(logits, dim=-1)\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    return probs\n",
        "\n",
        "  def get_loss(self, pred_probs, target_probs):\n",
        "    loss = None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.2)\n",
        "    # Hint:\n",
        "    # - As usual, the predictions are probabilities. But the labels for\n",
        "    #   sentiment are themselves probabilities. Since the targets are not be\n",
        "    #   single numbers, we cannot just use `F.cross_entropy`.\n",
        "    # - Therefore, you will need to implement cross entropy manually.\n",
        "    #     Refer to wikipedia: https://en.wikipedia.org/wiki/Cross_entropy\n",
        "    # - pred_logits shape: (batch_size, num_sentiment_class)\n",
        "    # - target_logits shape: (batch_size, num_sentiment_class)\n",
        "\n",
        "    loss = -torch.sum(target_probs * pred_probs, dim=-1).mean()\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfsmjWi3DADg"
      },
      "source": [
        "## **Task 4.3: Implement multi-task learning loss [5 points]**\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "**Metrics.** We provide code for computing the metrics for each possible auxiliary task.\n",
        "- For Task Type and Sentiment classification, use accuracy.\n",
        "- For Dialog Acts classification, use F1 score (since the labels are unbalanced).\n",
        "\n",
        "→ Instantiate all three auxiliary classifiers: `TaskTypeClassifier`, `DialogActsClassifier`, and `SentimentClassifier` in `__init__` for `LightningCTCMTL`\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "self.task_type_model = TaskTypeClassifier(...)\n",
        "self.dialogue_type_model = DialogActsClassifier(...)\n",
        "self.sentiment_type_model = SentimentClassifier(...)\n",
        "```\n",
        "\n",
        "**→ Implement/modify `get_multi_task_loss`.**\n",
        "\n",
        "In `LightningCTCMTL`, the available weights are:\n",
        "\n",
        "- `asr_weight`\n",
        "- `task_type_weight`\n",
        "- `dialog_acts_weight`\n",
        "- `sentiment_weight`\n",
        "\n",
        "You must:\n",
        "- Combine the add the primary ASR loss and the auxiliary task losses.\n",
        "- Use weighting parameters to balance the importance of different tasks during training.\n",
        "- Your weights should sum to 1 across the tasks you are training.\n",
        "\n",
        "Reminder:\n",
        "Every classifier you implemented above already includes a `get_loss` method.\n",
        "Use the `get_loss` method from each classifier to compute the corresponding auxiliary task losses in `get_multi_task_loss` below.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Choosing your loss weights is an important design decision.\n",
        "- You can adjust weights to make some tasks more important than others.\n",
        "- In some cases, you might even use negative weights to turn an auxiliary task into an adversarial task — meaning the model is encouraged not to perform well on that task.\n",
        "(This can sometimes improve the primary task's robustness on certain edge cases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5cllqMeW_pb"
      },
      "outputs": [],
      "source": [
        "class LightningCTCMTL(LightningCTC):\n",
        "  \"\"\"PyTorch Lightning class for training CTC with multi-task learning.\"\"\"\n",
        "  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128,\n",
        "               wav_max_length=200, transcript_max_length=200,\n",
        "               learning_rate=1e-3, batch_size=256, weight_decay=1e-5,\n",
        "               encoder_num_layers=2, encoder_hidden_dim=256,\n",
        "               encoder_bidirectional=True, asr_weight=1.0, task_type_weight=1.0,\n",
        "               dialog_acts_weight=1.0, sentiment_weight=1.0):\n",
        "    super().__init__(\n",
        "      n_mels=n_mels, hop_length=hop_length,\n",
        "      wav_max_length=wav_max_length,\n",
        "      transcript_max_length=transcript_max_length,\n",
        "      learning_rate=learning_rate,\n",
        "      batch_size=batch_size,\n",
        "      weight_decay=weight_decay,\n",
        "      encoder_num_layers=encoder_num_layers,\n",
        "      encoder_hidden_dim=encoder_hidden_dim,\n",
        "      encoder_bidirectional=encoder_bidirectional)\n",
        "    self.save_hyperparameters()\n",
        "    self.asr_weight = asr_weight\n",
        "    self.task_type_weight = task_type_weight\n",
        "    self.dialog_acts_weight = dialog_acts_weight\n",
        "    self.sentiment_weight = sentiment_weight\n",
        "\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.3)\n",
        "    # Instantiate your auxiliary task models here.\n",
        "\n",
        "    # Get the encoder output dimension\n",
        "    encoder_output_dim = encoder_hidden_dim * (2 if encoder_bidirectional else 1)\n",
        "\n",
        "    # You'll need to determine the actual number of classes from your dataset\n",
        "    # These are placeholder values - adjust based on your actual dataset\n",
        "    self.task_type_model = TaskTypeClassifier(encoder_output_dim, task_type_num_class)\n",
        "    self.dialog_acts_model = DialogActsClassifier(encoder_output_dim, dialog_acts_num_class)\n",
        "    self.sentiment_model = SentimentClassifier(encoder_output_dim, 3)  # 3 sentiment classes\n",
        "\n",
        "    ############################# END OF YOUR CODE #############################\n",
        "\n",
        "  def create_datasets(self):\n",
        "    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
        "    train_dataset = HarperValleyBankMTL(\n",
        "      root, split='train', n_mels=self.n_mels, n_fft=self.n_fft,\n",
        "      win_length=self.win_length, hop_length=self.hop_length,\n",
        "      wav_max_length=self.wav_max_length,\n",
        "      transcript_max_length=self.transcript_max_length,\n",
        "      append_eos_token=False)\n",
        "    val_dataset = HarperValleyBankMTL(\n",
        "      root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n",
        "      win_length=self.win_length, hop_length=self.hop_length,\n",
        "      wav_max_length=self.wav_max_length,\n",
        "      transcript_max_length=self.transcript_max_length,\n",
        "      append_eos_token=False)\n",
        "    test_dataset = HarperValleyBankMTL(\n",
        "      root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n",
        "      win_length=self.win_length, hop_length=self.hop_length,\n",
        "      wav_max_length=self.wav_max_length,\n",
        "      transcript_max_length=self.transcript_max_length,\n",
        "      append_eos_token=False)\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "  def get_multi_task_loss(self, batch, split='train'):\n",
        "    \"\"\"Gets losses and metrics for all task heads.\"\"\"\n",
        "    # Compute loss on the primary ASR task.\n",
        "    asr_loss, asr_metrics, embedding = self.get_primary_task_loss(batch, split)\n",
        "\n",
        "    # Note: Not all of these have to be used (it is up to your design)\n",
        "    task_type_labels = None\n",
        "    dialog_acts_labels = None\n",
        "    sentiment_labels = None\n",
        "    task_type_log_probs = None\n",
        "    dialog_acts_probs = None\n",
        "    sentiment_log_probs = None\n",
        "    task_type_loss = None\n",
        "    dialog_acts_loss = None\n",
        "    sentiment_loss = None\n",
        "    combined_loss = None\n",
        "    ############################ START OF YOUR CODE ############################\n",
        "    # TODO(4.3)\n",
        "    # Implement multi-task learning by combining multiple objectives.\n",
        "    # Define `combined_loss` here.\n",
        "\n",
        "    # Extract auxiliary labels from batch\n",
        "    # Batch format: (input_features, input_lengths, labels, label_lengths,\n",
        "    #                task_type_labels, dialog_acts_labels, sentiment_labels)\n",
        "    task_type_labels = batch[4]\n",
        "    dialog_acts_labels = batch[5]\n",
        "    sentiment_labels = batch[6]\n",
        "\n",
        "    # Get predictions from auxiliary models\n",
        "    # Use mean pooling over sequence dimension for embedding\n",
        "    pooled_embedding = torch.mean(embedding, dim=1)  # (batch_size, hidden_dim)\n",
        "\n",
        "    task_type_log_probs = self.task_type_model(pooled_embedding)\n",
        "    dialog_acts_probs = self.dialog_acts_model(pooled_embedding)\n",
        "    sentiment_log_probs = self.sentiment_model(pooled_embedding)\n",
        "\n",
        "    # Compute auxiliary losses\n",
        "    task_type_loss = self.task_type_model.get_loss(task_type_log_probs, task_type_labels)\n",
        "    dialog_acts_loss = self.dialog_acts_model.get_loss(dialog_acts_probs, dialog_acts_labels)\n",
        "    sentiment_loss = self.sentiment_model.get_loss(sentiment_log_probs, sentiment_labels)\n",
        "\n",
        "    # Combine all losses with weights\n",
        "    combined_loss = (self.asr_weight * asr_loss +\n",
        "                    self.task_type_weight * task_type_loss +\n",
        "                    self.dialog_acts_weight * dialog_acts_loss +\n",
        "                    self.sentiment_weight * sentiment_loss)\n",
        "\n",
        "    ############################ END OF YOUR CODE ##############################\n",
        "\n",
        "    with torch.no_grad():\n",
        "      ############################ START OF YOUR CODE ##########################\n",
        "      # TODO(4.3)\n",
        "      # No additional code is required here. :)\n",
        "      # We provide how to compute metrics for all possible auxiliary tasks and\n",
        "      # store them in your metrics dictionary. Comment out the metrics for tasks\n",
        "      # you do not plan to use.\n",
        "\n",
        "      # TASK_TYPE: Compare predicted task type to true task type.\n",
        "      task_type_preds = torch.argmax(task_type_log_probs, dim=1)\n",
        "      task_type_acc = \\\n",
        "        (task_type_preds == task_type_labels).float().mean().item()\n",
        "\n",
        "      # DIALOG_ACTS: Compare predicted dialog actions to true dialog actions.\n",
        "      dialog_acts_preds = torch.round(dialog_acts_probs)\n",
        "      dialog_acts_f1 = f1_score(dialog_acts_labels.cpu().numpy().reshape(-1),\n",
        "                                dialog_acts_preds.cpu().numpy().reshape(-1))\n",
        "\n",
        "\n",
        "      # # SENTIMENT: Compare largest predicted sentiment to largest true sentim\n",
        "      sentiment_preds = torch.argmax(sentiment_log_probs, dim=1)\n",
        "      sentiment_labels = torch.argmax(sentiment_labels, dim=1)\n",
        "      sentiment_acc = \\\n",
        "      (sentiment_preds == sentiment_labels).float().mean().item()\n",
        "\n",
        "      metrics = {\n",
        "        # Task losses.\n",
        "        f'{split}_asr_loss': asr_metrics[f'{split}_loss'],\n",
        "        f'{split}_task_type_loss': task_type_loss,\n",
        "        f'{split}_dialog_acts_loss': dialog_acts_loss,\n",
        "        f'{split}_sentiment_loss': sentiment_loss,\n",
        "        # CER as ASR metric.\n",
        "       f'{split}_asr_cer': asr_metrics[f'{split}_cer'],\n",
        "        # Accuracy as task_type metric.\n",
        "       f'{split}_task_type_acc': task_type_acc,\n",
        "        # F1 score as dialog_acts metric.\n",
        "        f'{split}_dialog_acts_f1': dialog_acts_f1,\n",
        "        # # Accuracy as sentiment metric.\n",
        "        f'{split}_sentiment_acc': sentiment_acc\n",
        "      }\n",
        "      ############################ END OF YOUR CODE ############################\n",
        "    return combined_loss, metrics\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    parameters = chain(self.model.parameters(),\n",
        "                       self.task_type_model.parameters(),\n",
        "                      #  self.dialog_acts_model.parameters(),\n",
        "                      #  self.sentiment_model.parameters()\n",
        "                      )\n",
        "    optim = torch.optim.AdamW(parameters, lr=self.lr,\n",
        "                              weight_decay=self.weight_decay)\n",
        "    return [optim], []\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "      loss, metrics = self.get_multi_task_loss(batch, split='train')\n",
        "      # log every train metric; Lightning will avg across steps & epochs\n",
        "      for k, v in metrics.items():\n",
        "          self.log(\n",
        "              k, v,\n",
        "              on_step=True, on_epoch=True,\n",
        "              prog_bar=('asr' in k), sync_dist=True\n",
        "          )\n",
        "      return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "      loss, metrics = self.get_multi_task_loss(batch, split='val')\n",
        "      # log every val metric; Lightning will avg across all val batches\n",
        "      for k, v in metrics.items():\n",
        "          self.log(\n",
        "              k, v,\n",
        "              on_step=False, on_epoch=True,\n",
        "              prog_bar=('cer' in k), sync_dist=True\n",
        "          )\n",
        "      return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "      loss, metrics = self.get_multi_task_loss(batch, split='test')\n",
        "      # log every test metric; Lightning will avg across all test batches\n",
        "      for k, v in metrics.items():\n",
        "          self.log(\n",
        "              k, v,\n",
        "              on_step=False, on_epoch=True,\n",
        "              prog_bar=('cer' in k), sync_dist=True\n",
        "          )\n",
        "      return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7x88wh0WTj4"
      },
      "source": [
        "## **Task 4.4: Training CTC-MTL [20 points]**\n",
        "\n",
        "**Training & Written Response**\n",
        "\n",
        "**→ Train the CTC-MTL network with the default hyperparameters we provide.**\n",
        "\n",
        "One epoch of training CTC-MTL takes 3 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n",
        "\n",
        "**CER target:\n",
        "You should obtain a test CER of at most 0.30 for this model. You will obtain full points for demonstrating a model with test CER below this threshold.**\n",
        "\n",
        "**→ Paste screenshots from your Weights & Biases dashboard of your loss curves and CER curve in the cell marked \"Plots\". Remember to include learning curves for the auxiliary tasks!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4D8kJJKxWVK"
      },
      "outputs": [],
      "source": [
        "# Run CTC-MTL\n",
        "\n",
        "config = {\n",
        "  'n_mels': 128,\n",
        "  'n_fft': 256,\n",
        "  'win_length': 256,\n",
        "  'hop_length': 128,\n",
        "  'wav_max_length': 512,\n",
        "  'transcript_max_length': 200,\n",
        "  'learning_rate': 1e-3,\n",
        "  'batch_size': 128,\n",
        "  'weight_decay': 0,\n",
        "  'encoder_num_layers': 2,\n",
        "  'encoder_hidden_dim': 256,\n",
        "  'encoder_bidirectional': True,\n",
        "  # you may wish to play with these weights; try to keep the sum\n",
        "  # of them equal to one.\n",
        "  'asr_weight': 0.25,\n",
        "  'task_type_weight': 0.25,\n",
        "  'dialog_acts_weight': 0.25,\n",
        "  'sentiment_weight': 0.25,\n",
        "}\n",
        "\n",
        "run(system=\"LightningCTCMTL\", config=config, ckpt_dir='ctc_mtl', epochs=20,\n",
        "    monitor_key='val_loss', use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrqzM4TTleFx"
      },
      "outputs": [],
      "source": [
        "# You can find the saved checkpoint here:\n",
        "!ls /content/cs224s_spring2025/trained_models/ctc_mtl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYCTmwiNkioM"
      },
      "source": [
        "**→ Using your plots as evidence in your description, answer the following questions:**\n",
        "\n",
        "a) Report performance metrics on each of the auxiliary tasks and the CER of your jointly trained model.\n",
        "\n",
        "b) Under the same configuration of hyperparameters, does CTC-MTL perform better than CTC? Why or why not? (Hint: Have the loss plots converged? How does multi-tasking affect the speed of learning the primary task?)\n",
        "\n",
        "c) Which tasks seem to be more difficult than others? Why might that be?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ1KkpAGlDX1"
      },
      "source": [
        "---\n",
        "\n",
        "**Plots:**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBbCzn0512GV"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMhSTNY-Gb0-"
      },
      "source": [
        "# Part 5: One Model to Hear Them All\n",
        "Congratulations, by this point you have trained multiple end-to-end deep learning neural networks for automatic speech recognition!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8nXgReDYn4M"
      },
      "source": [
        "## **Task 5.1: Train and summarize your best model [30 Points]**\n",
        "\n",
        "**Training & Written Response**\n",
        "\n",
        "**Note:** You are welcome to conduct additional experiments in this part. Please copy cells from above into Part 7 of the notebook if you wish to utilize them.\n",
        "\n",
        "**→ Alter any model of your choice or training procedure to improve performance! Describe what you tried, and report performance of your best model.** Include in your answer your design choices of:\n",
        "- Type of loss functions (CTC or Joint CTC-MTL)\n",
        "- Any auxiliary task(s) and weighting of tasks you may have used\n",
        "- Training hyperparameters (e.g. learning rate)\n",
        "\n",
        "**Hints for choosing auxiliary tasks:**\n",
        "You must experiment with different combinations of auxiliary tasks to find the best-performing model.\n",
        "Out of the three auxiliary tasks (`TaskTypeClassifier`, `DialogActsClassifier`, and `SentimentClassifier`), you are expected to explore and determine which combination improves your ASR model the most.\n",
        "\n",
        "To experiment with different task combinations:\n",
        "- Revisit `LightningCTCMTL`\n",
        "  - In `__init__`, only instantiate the classifiers for the auxiliary tasks you want to use.\n",
        "  - In `get_multi_task_loss`, always include ASR loss (since that's the primary task), and add only the auxiliary task losses you want to use.\n",
        "- In Part 4.4: when you change the active tasks, you must update your weights so that the weights for all active tasks, including ASR, sum to 1.\n",
        "\n",
        "You should attempt to improve your model performance in a reasonable way given what you have observed so far. You do not need to exhaustively optimize performance though; training at least 2 new models with improvements from previous model.\n",
        "\n",
        "\n",
        "**CER target:\n",
        "You should obtain a test CER of at most 0.25 for this model. You will earn a majority of the points for demonstrating a model with test CER below this threshold. Teaching staff will vote on top-performing systems and give full credit to only the top ~10 systems in the course.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0g5FHJA-Mu0"
      },
      "outputs": [],
      "source": [
        "#### TODO YOUR CODE HERE ####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh7lhrPAb_u1"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLN0ItdPrtwP"
      },
      "source": [
        "**Great work!** You have completed the third assignment of the course, and in doing so you have 1) trained deep acoustic models from scratch on a speech dataset, 2) gained a practical intuition for different architectures and design choices for ASR, and 3) assessed a speech recognition system on your own voice!\n",
        "\n",
        "**Gradescope Submission**\n",
        "- Download your Colab notebook **with all cells fully executed** as a `.ipynb` file. Zip together your `.ipynb` with any supporting files. Submit this zipped file under `Assignment 3: Code Submission`.\n",
        "- Open your `.ipynb` file locally and save it as a PDF. Submit this PDF under `Assignment 3: PDF Submission` and **tag all pages corresponding to each task**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA-eoZYubhSd"
      },
      "source": [
        "# Optional: Testing Your System!\n",
        "\n",
        "**You can test your system with your own voice samples! Make 2 short recordings of yourself: 1) one on any topic and 2) another on a topic that more closely matches utterances in the HarperValleyBank dataset.** Save/convert them as `.WAV` files and upload them to your `DATA_PATH` directory. Then use `run_inference` to get your best model's transcriptions on them.\n",
        "\n",
        "- How does it do? Are the transcripts accurate?\n",
        "- Does the model generalize? If not, why do you think that is? What changes could be made to help the model generalize better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JMjmSXy1qO4"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npf3piJcidS6"
      },
      "outputs": [],
      "source": [
        "# Load your saved model weights.\n",
        "\n",
        "ctc_checkpoint_file ='epoch=19-step=1620-v1.ckpt' # Fill in your checkpoint file\n",
        "ctc_checkpoint_path = os.path.join(MODEL_PATH, 'ctc', ctc_checkpoint_file)\n",
        "\n",
        "system = None\n",
        "wav = None\n",
        "device = torch.device('cuda')\n",
        "sr = 8000\n",
        "\n",
        "# Use system.eval() after you and your PyTorch Lightning system weights.\n",
        "# Use `librosa.load` (refer to https://librosa.org/doc/latest/index.html).\n",
        "# Use the default target sample rate of 8000.\n",
        "# Use `librosa.effects.trim` to remove leading and trailing silences.\n",
        "\n",
        "device = torch.device('cuda')\n",
        "system = LightningCTC.load_from_checkpoint(ctc_checkpoint_path)\n",
        "system = system.to(device)\n",
        "system.eval()\n",
        "wav_bank, sr = librosa.load(f'{DATA_PATH}/wav_bank.wav', sr=8000)\n",
        "wav_normal, sr = librosa.load(f'{DATA_PATH}/wav_normal.wav', sr=8000)\n",
        "\n",
        "wav_bank, _   = librosa.effects.trim(wav_bank)\n",
        "wav_normal, _ = librosa.effects.trim(wav_normal)\n",
        "\n",
        "predicted_bank = run_inference(system, wav_bank, device=device, sr=sr)\n",
        "predicted_normal= run_inference(system, wav_normal, device=device, sr=sr)\n",
        "\n",
        "print(\"Bank transcript:  \", predicted_bank)\n",
        "print(\"Normal transcript:\", predicted_normal)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5967f166116f42e9a994d27bb3b8f0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_530208248c8f4b77b1935893c3a4d9f9",
              "IPY_MODEL_2cd94efbaae54fa6b0377f42fd5d6fdb",
              "IPY_MODEL_517825ac83844f7083ee23b0406745ba"
            ],
            "layout": "IPY_MODEL_caa2b2fd48e64e1a8b1cfa9bdc0f2758"
          }
        },
        "530208248c8f4b77b1935893c3a4d9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec6f70996d4a4cdfa056d01cc701f72d",
            "placeholder": "​",
            "style": "IPY_MODEL_4caadfbb66194d1d8106cf3b06ff1579",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "2cd94efbaae54fa6b0377f42fd5d6fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ea65c0f0e5f45dbb74a43fffd7176a5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c204b2e5661406380d089b922f86337",
            "value": 2
          }
        },
        "517825ac83844f7083ee23b0406745ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b147b8ffbf174a75a8bb5ff307ad9c46",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a22e859bbe4dd68ceb141682af819f",
            "value": " 2/2 [00:08&lt;00:00,  0.23it/s]"
          }
        },
        "caa2b2fd48e64e1a8b1cfa9bdc0f2758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "ec6f70996d4a4cdfa056d01cc701f72d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4caadfbb66194d1d8106cf3b06ff1579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ea65c0f0e5f45dbb74a43fffd7176a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c204b2e5661406380d089b922f86337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b147b8ffbf174a75a8bb5ff307ad9c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a22e859bbe4dd68ceb141682af819f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a63d23beaa8468dbac47676dd8d7969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbbbb0786ff54c97b1548c4eb72fed3f",
              "IPY_MODEL_f5770451a9724cf4a4619121c1acef1e",
              "IPY_MODEL_8a1e10226f4845d6bd8018259bf26369"
            ],
            "layout": "IPY_MODEL_6f55e7398a114cb8ae926e1a44cd687a"
          }
        },
        "cbbbb0786ff54c97b1548c4eb72fed3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_907be565f3e841b789962ca3aa6b518d",
            "placeholder": "​",
            "style": "IPY_MODEL_ad2fdaa32569432fac39a68593cf49e4",
            "value": "Epoch 5:   0%"
          }
        },
        "f5770451a9724cf4a4619121c1acef1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51254190d3f4adba517799a6ab6892e",
            "max": 81,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79942ec59935415da9c4d44a806f3c51",
            "value": 0
          }
        },
        "8a1e10226f4845d6bd8018259bf26369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d96ff8af941a4a88850a365ad9a1086d",
            "placeholder": "​",
            "style": "IPY_MODEL_40e5618ab4b64240acaf5691e9fe41c5",
            "value": " 0/81 [01:29&lt;?, ?it/s, v_num=4vue, val_loss=2.930, val_cer=0.998]"
          }
        },
        "6f55e7398a114cb8ae926e1a44cd687a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "907be565f3e841b789962ca3aa6b518d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad2fdaa32569432fac39a68593cf49e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a51254190d3f4adba517799a6ab6892e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79942ec59935415da9c4d44a806f3c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d96ff8af941a4a88850a365ad9a1086d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40e5618ab4b64240acaf5691e9fe41c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b24813aa9dea4b249fab082079c646b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88710e47bbee440b93a74a9797df173c",
              "IPY_MODEL_83d0c4d326484c34ae4eb9817a906f16",
              "IPY_MODEL_0ebee3fbaa7645bbb2c0a16c0ca49bb9"
            ],
            "layout": "IPY_MODEL_a9c6a2156bef47f290b55d80e88cd4e6"
          }
        },
        "88710e47bbee440b93a74a9797df173c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_472d93a5969e42999f3b9a1bfb585f70",
            "placeholder": "​",
            "style": "IPY_MODEL_521e68cecfdf4e6286d3814d9dd2946c",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "83d0c4d326484c34ae4eb9817a906f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a86d8707b4fa486bbfb7280b1fdfa30d",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe006e87254e44acba2b3613d2d13124",
            "value": 6
          }
        },
        "0ebee3fbaa7645bbb2c0a16c0ca49bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_334e21bd1b094c9a8a25d85e1af99cb4",
            "placeholder": "​",
            "style": "IPY_MODEL_6a21d49482b64c9580a2055eab42f97e",
            "value": " 6/6 [00:21&lt;00:00,  0.28it/s]"
          }
        },
        "a9c6a2156bef47f290b55d80e88cd4e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "472d93a5969e42999f3b9a1bfb585f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "521e68cecfdf4e6286d3814d9dd2946c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a86d8707b4fa486bbfb7280b1fdfa30d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe006e87254e44acba2b3613d2d13124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "334e21bd1b094c9a8a25d85e1af99cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a21d49482b64c9580a2055eab42f97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1c2f28c8a048f2aad1f5b8b824f411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c9d1ae9cab6439e8179020226a9ded5",
              "IPY_MODEL_f4c348beffed4a8980a6d39e73be9678",
              "IPY_MODEL_94ee98624a3b49e39a3313520c3f9716"
            ],
            "layout": "IPY_MODEL_25f10fc30b7c4415af81db86b74e2e1b"
          }
        },
        "0c9d1ae9cab6439e8179020226a9ded5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e39672ead1c4257abbdd55b879974cc",
            "placeholder": "​",
            "style": "IPY_MODEL_3ac6bb040261450fb40d9f8c03b13aed",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "f4c348beffed4a8980a6d39e73be9678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c609f8239e141eabb4d67b3fedf1557",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc0ceec353114203bfcd2ffa77217323",
            "value": 6
          }
        },
        "94ee98624a3b49e39a3313520c3f9716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_177e23603dd8494486ac4c70ddc1ff0c",
            "placeholder": "​",
            "style": "IPY_MODEL_3769bc338a694ad2afccb6fecb11b0ac",
            "value": " 6/6 [00:21&lt;00:00,  0.28it/s]"
          }
        },
        "25f10fc30b7c4415af81db86b74e2e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "6e39672ead1c4257abbdd55b879974cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ac6bb040261450fb40d9f8c03b13aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c609f8239e141eabb4d67b3fedf1557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0ceec353114203bfcd2ffa77217323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "177e23603dd8494486ac4c70ddc1ff0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3769bc338a694ad2afccb6fecb11b0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba87da05b7648328aa23d54d8ffe2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f31a073cd9747f384cbdddcdd2ce3e3",
              "IPY_MODEL_61e094acc75043c9975fecafed0c17bb",
              "IPY_MODEL_88d45b4f89e64642ab487c6f4161add1"
            ],
            "layout": "IPY_MODEL_afd8371e886741c8aae051ca41aafbbb"
          }
        },
        "5f31a073cd9747f384cbdddcdd2ce3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb3373dfc20f4dddad3ab7b88cc9bef1",
            "placeholder": "​",
            "style": "IPY_MODEL_177d5b681e9b467b80fd64979eb943b6",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "61e094acc75043c9975fecafed0c17bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae840da533c04ea28c1ce449d6e8a5b5",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_826d67a85061401aa985bf020f41ae2d",
            "value": 6
          }
        },
        "88d45b4f89e64642ab487c6f4161add1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1e483f138b5429cb899d6e779b1a48d",
            "placeholder": "​",
            "style": "IPY_MODEL_d9a09833f8e5421ea8e604f7e5260c46",
            "value": " 6/6 [00:22&lt;00:00,  0.27it/s]"
          }
        },
        "afd8371e886741c8aae051ca41aafbbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "bb3373dfc20f4dddad3ab7b88cc9bef1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177d5b681e9b467b80fd64979eb943b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae840da533c04ea28c1ce449d6e8a5b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "826d67a85061401aa985bf020f41ae2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1e483f138b5429cb899d6e779b1a48d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a09833f8e5421ea8e604f7e5260c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b9e9ce86ca24d9face6e77eac0f1c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_737c96bba79a4d2b8b9170bc1f895604",
              "IPY_MODEL_034beff7e1eb4d9fbaedd03de184784b",
              "IPY_MODEL_5c47fcf33d1441678729a670dc8bc93f"
            ],
            "layout": "IPY_MODEL_a103b699179f4684959f40a5485481b7"
          }
        },
        "737c96bba79a4d2b8b9170bc1f895604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bd144b00eb44e7096905e304232660e",
            "placeholder": "​",
            "style": "IPY_MODEL_76f5b9965b6e4337a6ae02c066d73875",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "034beff7e1eb4d9fbaedd03de184784b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09f7c795d13e498c82e94162829d5132",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54c288d8b8b04b4192241944c7a91ab4",
            "value": 6
          }
        },
        "5c47fcf33d1441678729a670dc8bc93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a48b7868efd44d7a5dadc407e94a336",
            "placeholder": "​",
            "style": "IPY_MODEL_d465b33b8dcc4e34a35883dc11bcb531",
            "value": " 6/6 [00:21&lt;00:00,  0.28it/s]"
          }
        },
        "a103b699179f4684959f40a5485481b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "7bd144b00eb44e7096905e304232660e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f5b9965b6e4337a6ae02c066d73875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09f7c795d13e498c82e94162829d5132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c288d8b8b04b4192241944c7a91ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a48b7868efd44d7a5dadc407e94a336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d465b33b8dcc4e34a35883dc11bcb531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1192c0cb494d4626a7c03ee96b20055d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe46499e0b2f4ca3b145feda2499298b",
              "IPY_MODEL_c10cf586d51c43a78a3e41320c925080",
              "IPY_MODEL_6f2d436367f94bf99bbe036409efb9bf"
            ],
            "layout": "IPY_MODEL_f048eccb3e2b4dacbe23ed655c3faa67"
          }
        },
        "fe46499e0b2f4ca3b145feda2499298b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4588a4da2aa74bbfaf81b10ed7420087",
            "placeholder": "​",
            "style": "IPY_MODEL_0de4952b02cd47e9a3b01fb31bca3c34",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "c10cf586d51c43a78a3e41320c925080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a61b38d5234b42ba8a7b50809d555c",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48caf4f3e01b4b488ced6e6a6644648e",
            "value": 6
          }
        },
        "6f2d436367f94bf99bbe036409efb9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d95e2b8becd548a6b33f983817651ca2",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a31279201543efba9c68cc8702b887",
            "value": " 6/6 [00:22&lt;00:00,  0.26it/s]"
          }
        },
        "f048eccb3e2b4dacbe23ed655c3faa67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "4588a4da2aa74bbfaf81b10ed7420087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0de4952b02cd47e9a3b01fb31bca3c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90a61b38d5234b42ba8a7b50809d555c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48caf4f3e01b4b488ced6e6a6644648e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d95e2b8becd548a6b33f983817651ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a31279201543efba9c68cc8702b887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}